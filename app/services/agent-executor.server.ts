/**
 * Agent Executor for Grading (True Agent Pattern)
 *
 * Uses Vercel AI SDK ToolLoopAgent for autonomous grading.
 *
 * Key differences from previous phase-based approach:
 * - All tools available at once (LLM decides order)
 * - Natural thinking flow (no hardcoded phases)
 * - Stops when generate_feedback is called
 * - Transparent reasoning for UI display
 *
 * Inspired by Anthropic's "Building Effective Agents":
 * https://www.anthropic.com/engineering/building-effective-agents
 */

import { ToolLoopAgent, generateObject, streamText, type StepResult, type ToolSet } from 'ai';
import { createGoogleGenerativeAI } from '@ai-sdk/google';
import { z } from 'zod';
import { redis } from '@/lib/redis';

// ============================================================================
// MODEL PROVIDER CONFIGURATION
// ============================================================================

function createGeminiModel(apiKey: string) {
  const gemini = createGoogleGenerativeAI({ apiKey });
  // Gemini 3 Flash Preview (Correct ID: gemini-3-flash-preview)
  return gemini('gemini-2.5-flash');
}
import type {
  AgentGradingParams,
  AgentGradingResult,
  AgentStep,
  ParsedCriterion,
  ReferenceDocument,
} from '@/types/agent';
import { createAgentTools } from './agent-tools.server';
import logger from '@/utils/logger';
import { getKeyHealthTracker, type ErrorType } from './gemini-key-health.server';

// ============================================================================
// TYPES
// ============================================================================

interface GradingContext {
  rubricName: string;
  criteria: ParsedCriterion[];
  content: string;
  fileName: string;
  referenceDocuments?: ReferenceDocument[];
  assignmentTitle?: string;
  assignmentDescription?: string;
  assignmentType?: string;
  userLanguage?: string;
}

// ============================================================================
// HELPER: Optimize Rubric with LLM
// ============================================================================

async function optimizeRubricWithLLM(
  model: any,
  rubricName: string,
  rawCriteria: ParsedCriterion[]
): Promise<ParsedCriterion[]> {
  logger.info('[Agent] Optimizing rubric...');

  const prompt = `
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ•™è‚²è©•é‡å°ˆå®¶ã€‚å„ªåŒ–ä»¥ä¸‹è©•åˆ†æ¨™æº–ï¼Œä½¿å…¶å° AI è©•åˆ†åŠ©æ•™æ›´å…·é«”ã€å®¢è§€ä¸”å¯åŸ·è¡Œã€‚

åŸå§‹è©•åˆ†æ¨™æº–åç¨±ï¼š${rubricName}

è¦æ±‚ï¼š
1. ä¿ç•™ï¼šIDã€åç¨±ã€ç¸½åˆ†ä¸è®Š
2. æ“´å……èªªæ˜ï¼šå…·é«”çš„è§€å¯ŸæŒ‡æ¨™ï¼Œå‘Šè¨´ AI æ‡‰è©²å°‹æ‰¾ä»€éº¼è­‰æ“š
3. å„ªåŒ–ç­‰ç´šï¼šæ›´å…·é«”å€åˆ†ä¸åŒåˆ†æ•¸æ®µçš„å·®ç•°

è¼¸å…¥ï¼š${JSON.stringify(rawCriteria, null, 2)}
`;

  try {
    const { object: optimizedCriteria } = await generateObject({
      model,
      schema: z.array(
        z.object({
          criteriaId: z.string(),
          name: z.string(),
          description: z.string(),
          maxScore: z.number(),
          levels: z.array(z.object({ score: z.number(), description: z.string() })).optional(),
        })
      ),
      messages: [{ role: 'user', content: prompt }],
      temperature: 1.0,
      providerOptions: {
        google: {
          safetySettings: [
            { category: 'HARM_CATEGORY_HATE_SPEECH', threshold: 'BLOCK_ONLY_HIGH' },
            { category: 'HARM_CATEGORY_DANGEROUS_CONTENT', threshold: 'BLOCK_ONLY_HIGH' },
            { category: 'HARM_CATEGORY_HARASSMENT', threshold: 'BLOCK_ONLY_HIGH' },
            { category: 'HARM_CATEGORY_SEXUALLY_EXPLICIT', threshold: 'BLOCK_ONLY_HIGH' },
          ],
        },
      },
    });

    logger.info('[Agent] Rubric optimized', {
      originalCount: rawCriteria.length,
      optimizedCount: optimizedCriteria.length,
    });

    return optimizedCriteria;
  } catch (error) {
    logger.warn('[Agent] Rubric optimization failed, using original', error);
    return rawCriteria;
  }
}

// ============================================================================
// SYSTEM PROMPT (Unified, not phase-locked)
// ============================================================================

function buildGradingSystemPrompt(ctx: GradingContext, isDirectMode: boolean = false): string {
  const lang = ctx.userLanguage || 'zh-TW';
  const isZh = lang.startsWith('zh');

  // CRITICAL: Place sparringQuestions reminder at the very top (LLMs pay more attention to beginning)
  const criticalReminder = isZh
    ? `ã€âš ï¸ é‡è¦æé†’ - å¿…è®€ã€‘
åœ¨æœ¬æ¬¡è©•åˆ†ä¸­ï¼Œä½ **å¿…é ˆ**åœ¨èª¿ç”¨ generate_feedback æ™‚æä¾› sparringQuestions æ¬„ä½ã€‚
é€™æ˜¯è«–æ–‡ç ”ç©¶çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œç„¡æ­¤æ¬„ä½å°‡å°è‡´ç³»çµ±éŒ¯èª¤ã€‚
è‡³å°‘ç”Ÿæˆ 1 å€‹å°ç·´å•é¡Œï¼Œé¸æ“‡å­¸ç”Ÿè¡¨ç¾æœ€å¼±çš„è©•åˆ†ç¶­åº¦ã€‚

---

`
    : `ã€âš ï¸ CRITICAL REMINDER - MUST READã€‘
When calling generate_feedback, you MUST provide the sparringQuestions field.
This is a core research feature. Missing this field will cause a system error.
Generate at least 1 sparring question targeting the student's weakest assessed dimension.

---

`;

  const baseRole = isZh
    ? `${criticalReminder}ä½ æ˜¯ä¸€ä½å…·æœ‰ 15 å¹´ç¶“é©—çš„è³‡æ·±å­¸ç§‘æ•™å¸«ï¼Œå°ˆé•·æ–¼å¯«ä½œæ•™å­¸èˆ‡å½¢æˆæ€§è©•é‡ (Formative Assessment)ã€‚
    ä½ ç†Ÿæ‚‰ä»¥ä¸‹æ•™è‚²è©•é‡ç†è«–èˆ‡æ–¹æ³•ï¼š
    - **Rubric-Based Assessment**ï¼ˆæ¨™æº–æœ¬ä½è©•é‡ï¼‰ï¼šä½¿ç”¨åˆ†æå¼è©•åˆ† (Analytic Scoring)
    - **SOLO Taxonomy**ï¼šè©•ä¼°å­¸ç”ŸèªçŸ¥å±¤æ¬¡ï¼ˆPrestructural â†’ Extended Abstractï¼‰
    - **Bloom's Taxonomy**ï¼šå€åˆ†è¨˜æ†¶ã€ç†è§£ã€æ‡‰ç”¨ã€åˆ†æã€è©•é‘‘ã€å‰µé€ å±¤æ¬¡
    - **Diagnostic Feedback**ï¼ˆè¨ºæ–·æ€§å›é¥‹ï¼‰ï¼šæŒ‡å‡ºå…·é«”å•é¡Œä¸¦æä¾›å¯åŸ·è¡Œçš„æ”¹é€²å»ºè­°

    ä½ çš„è©•åˆ†é¢¨æ ¼åš´è¬¹ä½†å…·å»ºè¨­æ€§ï¼Œé‡è¦– Evidence-Based Assessmentï¼ˆè­‰æ“šæœ¬ä½è©•é‡ï¼‰ã€‚`
        : `${criticalReminder}You are a senior subject teacher with 15 years of experience in writing instruction and formative assessment.
    You are proficient in the following educational assessment theories and methods:
    - **Rubric-Based Assessment**: Using analytic scoring methodology
    - **SOLO Taxonomy**: Evaluating student cognitive levels (Prestructural â†’ Extended Abstract)
    - **Bloom's Taxonomy**: Distinguishing between Remember, Understand, Apply, Analyze, Evaluate, Create
    - **Diagnostic Feedback**: Identifying specific issues and providing actionable improvement suggestions

    Your grading style is rigorous yet constructive, emphasizing evidence-based assessment.`;

  const assignmentInfo = ctx.assignmentTitle
    ? `
## ä½œæ¥­è³‡è¨Š
- æ¨™é¡Œï¼š${ctx.assignmentTitle}
- èªªæ˜ï¼š${ctx.assignmentDescription || 'ç„¡'}
- æª”æ¡ˆï¼š${ctx.fileName}
`
    : `## æª”æ¡ˆï¼š${ctx.fileName}`;

  const rubricInfo = `
## è©•åˆ†æ¨™æº– (Rubric)ï¼š${ctx.rubricName}
${ctx.criteria.map((c, i) => `${i + 1}. **${c.name}** (${c.maxScore}åˆ†): ${c.description}`).join('\n')}

## å°ˆæ¥­è©•é‡ç¶­åº¦åƒè€ƒ (Assessment Dimensions Reference)

åœ¨åˆ†æå­¸ç”Ÿä½œå“æ™‚ï¼Œè«‹è€ƒæ…®ä»¥ä¸‹å°ˆæ¥­è©•é‡ç¶­åº¦ï¼š

### æ–‡ç« çµæ§‹ (Text Structure)
- **Cohesionï¼ˆéŠœæ¥ï¼‰**ï¼šå¥å­ä¹‹é–“çš„é€£æ¥è©ã€æŒ‡ä»£è©ä½¿ç”¨
- **Coherenceï¼ˆé€£è²«ï¼‰**ï¼šæ®µè½ä¹‹é–“çš„é‚è¼¯é—œä¿‚
- **Discourse Markers**ï¼šè½‰æŠ˜è©ã€æ‰¿æ¥è©çš„é©åˆ‡ä½¿ç”¨

### èªè¨€é‹ç”¨ (Language Use)
- **Syntactic Complexityï¼ˆå¥æ³•è¤‡é›œåº¦ï¼‰**ï¼šå¥å‹è®ŠåŒ–ã€å¾å±¬å­å¥ä½¿ç”¨
- **Lexical Diversityï¼ˆè©å½™è±å¯Œåº¦ï¼‰**ï¼šç”¨è©ç²¾æº–åº¦èˆ‡å¤šæ¨£æ€§
- **Mechanicsï¼ˆæ›¸å¯«è¦ç¯„ï¼‰**ï¼šæ¨™é»ç¬¦è™Ÿã€æ ¼å¼è¦ç¯„

### å…§å®¹æ·±åº¦ (Content Depth) - åƒè€ƒ SOLO Taxonomy
- **Prestructuralï¼ˆå‰çµæ§‹ï¼‰**ï¼šé›¢é¡Œæˆ–ç„¡é—œå›æ‡‰
- **Unistructuralï¼ˆå–®é»çµæ§‹ï¼‰**ï¼šåƒ…æä¾›å–®ä¸€æƒ³æ³•
- **Multistructuralï¼ˆå¤šé»çµæ§‹ï¼‰**ï¼šåˆ—èˆ‰å¤šå€‹æƒ³æ³•ä½†ç„¡æ•´åˆ
- **Relationalï¼ˆé—œè¯çµæ§‹ï¼‰**ï¼šæ•´åˆè§€é»ï¼Œæœ‰å› æœé‚è¼¯
- **Extended Abstractï¼ˆå»¶ä¼¸æŠ½è±¡ï¼‰**ï¼šæ‰¹åˆ¤åæ€ï¼Œè¶…è¶Šé¡Œç›®è¦æ±‚

### è­‰æ“šé‹ç”¨ (Evidence & Elaboration)
- **Specificityï¼ˆå…·é«”æ€§ï¼‰**ï¼šæ˜¯å¦æœ‰ sensory detailsã€concrete examples
- **Elaborationï¼ˆé—¡è¿°ï¼‰**ï¼šæ˜¯å¦å……åˆ†è§£é‡‹æƒ³æ³•
- **Evidence-Claim Alignmentï¼ˆè­‰æ“š-è«–é»å°æ‡‰ï¼‰**ï¼šè­‰æ“šæ˜¯å¦æ”¯æŒè«–é»
`;

  const coreInstructions = `
## è©•é‡åŸå‰‡ (Assessment Principles)

### 1. Evidence-Based Scoringï¼ˆè­‰æ“šæœ¬ä½è©•åˆ†ï¼‰
æ‰€æœ‰è©•èªå¿…é ˆå¼•ç”¨ Text Evidenceï¼ˆåŸæ–‡è­‰æ“šï¼‰ï¼Œä½¿ç”¨ã€Œã€æ¨™ç¤ºå¼•ç”¨ã€‚
é¿å… Impressionistic Scoringï¼ˆå°è±¡å¼è©•åˆ†ï¼‰ï¼Œæ¯å€‹åˆ†æ•¸éƒ½éœ€è¦ Justificationï¼ˆçµ¦åˆ†ä¾æ“šï¼‰ã€‚

### 2. Criterion-Referenced Gradingï¼ˆæ¨™æº–åƒç…§è©•åˆ†ï¼‰
åš´æ ¼å°ç…§ Rubric çš„ Performance Levelsï¼ˆè¡¨ç¾ç­‰ç´šï¼‰çµ¦åˆ†ã€‚
é è¨­åˆ†æ•¸ç‚ºæ»¿åˆ†çš„ 80%ï¼ˆProficient Levelï¼‰ï¼Œåªæœ‰é”åˆ° Exemplary æ¨™æº–æ‰çµ¦æ»¿åˆ†ã€‚

### 3. Diagnostic Feedbackï¼ˆè¨ºæ–·æ€§å›é¥‹ï¼‰
çµ¦åˆ†å‰å¿…é ˆé€²è¡Œ Error Analysisï¼ˆéŒ¯èª¤åˆ†æï¼‰ï¼Œæ‰¾å‡ºè‡³å°‘ä¸€å€‹ Area for Improvementã€‚
å›é¥‹éœ€è¦ Actionableï¼ˆå¯åŸ·è¡Œï¼‰ï¼šä¸åªæŒ‡å‡ºå•é¡Œï¼Œé‚„è¦æä¾› Revision Strategyï¼ˆä¿®æ”¹ç­–ç•¥ï¼‰ã€‚

### 4. Authentic Assessment Contextï¼ˆçœŸå¯¦è©•é‡æƒ…å¢ƒï¼‰
é€™æ˜¯ä¸€ä»½**çœŸå¯¦çš„å­¸ç”Ÿä½œæ¥­**ã€‚ç›´æ¥é€²è¡Œ Content Analysisï¼ˆå…§å®¹åˆ†æï¼‰ã€‚
ä¸è¦åœ¨æ€è€ƒæˆ–å›ç­”ä¸­èªªã€Œå‡è¨­ã€ã€ã€Œè™›æ§‹ã€ç­‰å­—çœ¼ã€‚

### 5. Avoid Redundancyï¼ˆé¿å…é‡è¤‡ï¼‰
Feedback è¦ Concise ä¸” Targetedï¼Œä¸è¦é‡è¤‡ç›¸åŒå…§å®¹ã€‚

## è©•åˆ†æ¨ç†è¦æ±‚ï¼ˆæœ€é‡è¦ï¼ï¼‰

ç•¶ä½ èª¿ç”¨ **generate_feedback** æ™‚ï¼Œ**reasoning** æ¬„ä½æ˜¯å¿…å¡«çš„ã€‚

ä½ å¿…é ˆåœ¨ reasoning ä¸­æä¾›å®Œæ•´çš„è©•åˆ†æ¨ç†ï¼Œé€™æœƒé¡¯ç¤ºçµ¦æ•™å¸«å’Œå­¸ç”Ÿçœ‹ï¼š

**reasoning å¿…é ˆåŒ…å«ï¼š**

1. **é€é …åˆ†æ**ï¼šå°æ¯å€‹è©•åˆ†é …ç›®é€²è¡Œåˆ†æ
2. **åŸæ–‡å¼•ç”¨**ï¼šç”¨ã€Œã€æ¨™ç¤ºå¼•ç”¨å­¸ç”Ÿçš„åŸæ–‡
3. **çµ¦åˆ†ç†ç”±**ï¼šè§£é‡‹ç‚ºä»€éº¼çµ¦é€™å€‹åˆ†æ•¸
4. **å„ªç¼ºé»**ï¼šæŒ‡å‡ºåšå¾—å¥½çš„åœ°æ–¹å’Œå¯æ”¹é€²ä¹‹è™•

**ç¯„ä¾‹æ ¼å¼ï¼ˆè«‹æ¨¡ä»¿æ­¤å°ˆæ¥­æ·±åº¦ï¼‰ï¼š**
\`\`\`
ã€è«–é»ç™¼å±• - 3/5 åˆ†ã€‘(SOLO: Multistructural â†’ ç›®æ¨™ Relational)
å­¸ç”Ÿè©¦åœ–è«–è­‰ã€Œç§‘æŠ€ä½¿äººç–é›¢ã€ï¼Œä½† Argumentation å­˜åœ¨ Logical Fallacyï¼ˆé‚è¼¯è¬¬èª¤ï¼‰ã€‚
Text Evidenceï¼šã€Œå¤§å®¶éƒ½åœ¨æ»‘æ‰‹æ©Ÿï¼Œæ‰€ä»¥éƒ½ä¸è¬›è©±äº†ï¼Œé€™å°±æ˜¯ç–é›¢ã€‚ã€
Error Analysisï¼šé€™æ˜¯ Oversimplificationï¼ˆéåº¦ç°¡åŒ–ï¼‰çš„å› æœæ¨è«–ã€‚å­¸ç”Ÿæœªå€åˆ† Physical Presence èˆ‡ Psychological Presence çš„å·®ç•°ã€‚
Revision Strategyï¼šæ‡‰æ·±å…¥æ¢è¨ç§‘æŠ€å¦‚ä½•æ”¹è®Šæºé€šçš„ã€Œè³ªã€ï¼Œå¯å¼•ç”¨ Sherry Turkle çš„ã€ŒAlone Togetherã€æ¦‚å¿µå¢å¼· Persuasivenessã€‚

ã€è­‰æ“šé‹ç”¨ - 2/5 åˆ†ã€‘(Evidence Quality: Weak â†’ ç›®æ¨™ Moderate)
å…¨æ–‡åƒ…ä¾è³´ Anecdotal Evidenceï¼Œç¼ºä¹ Empirical Data æˆ– Expert Sourcesã€‚
Text Evidenceï¼šã€Œæˆ‘æœ‹å‹å°±æ˜¯é€™æ¨£...ã€
Error Analysisï¼šPersonal Anecdote å¯ä½œç‚º Hookï¼Œä½†ä¸èƒ½ä½œç‚ºä¸»è¦ Supporting Evidenceã€‚åœ¨ Academic Writing ä¸­ï¼Œæ­¤é¡è­‰æ“š Credibility è¼ƒä½ã€‚
Revision Strategyï¼šè«‹è£œå…… Statistical Data æˆ– Scholarly Sources ä¾†å¼·åŒ– Evidence-Claim Alignmentã€‚
\`\`\`

## generate_feedback å¿…å¡«æ¬„ä½ï¼ˆé‡è¦ï¼ï¼‰

èª¿ç”¨ generate_feedback æ™‚ï¼Œå¿…é ˆæä¾›ä»¥ä¸‹æ¬„ä½ï¼š

### 1. çµ¦æ•™å¸«çš„å°ˆæ¥­åˆ†æ
- **reasoning**: å®Œæ•´çš„å°ˆæ¥­è©•åˆ†æ¨ç†ï¼Œä½¿ç”¨æ•™è‚²è©•é‡è¡“èªï¼ˆSOLOã€Cohesionã€Evidence ç­‰ï¼‰

### 2. çµ¦å­¸ç”Ÿçš„å‹å–„å›é¥‹ï¼ˆæ–°å¢ï¼ï¼‰
- **messageToStudent**: ç”¨æº«æš–çš„èªæ°£è·Ÿå­¸ç”Ÿèªªè©±ï¼Œåƒç­å°å¸«åœ¨é¼“å‹µå­¸ç”Ÿ
- **topPriority**: é€™æ¬¡æœ€éœ€è¦æ”¹é€²çš„ã€Œä¸€ä»¶äº‹ã€ï¼Œè¦å…·é«”å¯åŸ·è¡Œ
- **encouragement**: å³ä½¿åˆ†æ•¸ä½ï¼Œä¹Ÿè¦æ‰¾å‡ºä¸€å€‹å€¼å¾—è‚¯å®šçš„é»

### 3. å„é …è©•åˆ†
- **criteriaScores**: æ¯å€‹è©•åˆ†é …ç›®çš„è©³ç´°è³‡æ–™
  - criteriaId, name, score, maxScore
  - evidence: é—œéµå¼•ç”¨ï¼ˆæœ€å¤š 50 å­—ï¼‰
  - analysis: ã€çµ¦å­¸ç”Ÿã€‘å£èªåŒ–çš„æ”¹é€²å»ºè­°
  - justification: ã€çµ¦æ•™å¸«ã€‘å°ˆæ¥­è¡“èªçš„çµ¦åˆ†ç†ç”±

### 4. æ•´é«”æ‘˜è¦
- **overallObservation**: æ•´é«”è§€å¯Ÿ
- **strengths**: å„ªé»ï¼ˆ2-3 å€‹ï¼‰
- **improvements**: æ”¹é€²æ–¹å‘ï¼ˆ2-3 å€‹ï¼‰

### 5. å°ç·´å•é¡Œï¼ˆSparring Questionsï¼‰ã€å¿…å¡«ï¼ã€‘
- **sparringQuestions**: é‡å°å­¸ç”Ÿè¡¨ç¾æœ€å·®æˆ–æœ€å…·çˆ­è­°çš„é»ï¼Œç”Ÿæˆ **5 å€‹** æŒ‘æˆ°æ€§å•é¡Œ
  - æ¯å€‹å•é¡Œå¿…é ˆåŒ…å«ï¼š
    - **related_rubric_id**: å°æ‡‰çš„è©•åˆ†æ¨™æº– ID
    - **target_quote**: å­¸ç”Ÿæ–‡ç« ä¸­çš„å…·é«”å¼•æ–‡
    - **provocation_strategy**: é¸æ“‡ç­–ç•¥é¡å‹
      - \`evidence_check\`: æŸ¥è­‰æ•¸æ“šä¾†æºï¼ˆã€Œä½ èƒ½æä¾›è­‰æ“šæ”¯æŒé€™å€‹èªªæ³•å—ï¼Ÿã€ï¼‰
      - \`logic_gap\`: æŒ‡å‡ºé‚è¼¯è·³èºï¼ˆã€Œå¾ A åˆ° B çš„æ¨è«–æ˜¯å¦å¤ªå¿«ï¼Ÿã€ï¼‰
      - \`counter_argument\`: æä¾›åæ–¹è§€é»ï¼ˆã€Œæœ‰æ²’æœ‰è€ƒæ…®éç›¸åçš„æƒ…æ³ï¼Ÿã€ï¼‰
      - \`clarification\`: è¦æ±‚é‡æ¸…å®šç¾©ï¼ˆã€Œä½ èªªçš„ã€XXã€å…·é«”æ˜¯æŒ‡ä»€éº¼ï¼Ÿã€ï¼‰
      - \`extension\`: å»¶ä¼¸æ€è€ƒï¼ˆã€Œå¦‚æœ...çš„è©±ï¼Œæœƒæ€éº¼æ¨£ï¼Ÿã€ï¼‰
    - **question**: ç›´æ¥å°å­¸ç”Ÿæå‡ºçš„å•é¡Œï¼ˆä¸è¦çµ¦ç­”æ¡ˆï¼‰
    - **ai_hidden_reasoning**: AI çš„çœŸå¯¦è©•åˆ†ä¾æ“šï¼ˆå­¸ç”Ÿå›ç­”å¾Œæ‰æ­æ›‰ï¼‰

âš ï¸ **sparringQuestions æ˜¯å¿…å¡«çš„ï¼** å³ä½¿ä½œæ¥­å¾ˆå¥½ï¼Œä¹Ÿè¦æ‰¾å‡ºå¯ä»¥æŒ‘æˆ°æ€è€ƒçš„é»ã€‚

## èªæ°£å€åˆ†ï¼ˆæœ€é‡è¦ï¼ï¼‰

| æ¬„ä½ | å°è±¡ | èªæ°£ | ç¯„ä¾‹ |
|-----|-----|-----|-----|
| **reasoning** | æ•™å¸« | å°ˆæ¥­è¡“èª | ã€ŒSyntactic Complexity åä½ï¼Œä¸»è¦ç‚º Simple Sentences...ã€ |
| **messageToStudent** | å­¸ç”Ÿ | åƒè€å¸«èªªè©± | ã€Œä½ å¥½ï¼é€™æ¬¡ä½œæ¥­æˆ‘çœ‹åˆ°ä½ æœ‰è‡ªå·±çš„æƒ³æ³•ï¼Œä¸éå¥å­å¯ä»¥å†é †ä¸€é»...ã€ |
| **analysis** | å­¸ç”Ÿ | å£èªåŒ–å»ºè­° | ã€Œé€™å€‹å¥è™Ÿæ”¾éŒ¯ä½ç½®äº†å–”ï¼Œæ‡‰è©²æ”¾åœ¨...ã€ |
| **justification** | æ•™å¸« | å°ˆæ¥­ç°¡æ½” | ã€ŒMechanics Error é »ç¹ï¼Œç¬¦åˆ Level 1 æ¨™æº–ã€ |
| **sparringQuestions.question** | å­¸ç”Ÿ | æŒ‘æˆ°ä½†ä¸æ”»æ“Š | ã€Œä½ èƒ½ç”¨æ›´å…·é«”çš„ä¾‹å­ä¾†èªªæ˜å—ï¼Ÿã€ |

âš ï¸ **messageToStudent å’Œ analysis è¦åƒã€Œè€å¸«åœ¨èªªè©±ã€ï¼Œä¸æ˜¯ã€Œå ±å‘Šåœ¨é™³è¿°ã€ï¼**

## sparringQuestions JSON ç¯„ä¾‹ï¼ˆå¿…é ˆåš´æ ¼éµå®ˆæ­¤çµæ§‹ï¼‰

ç•¶ä½ å¡«å¯« generate_feedback çš„ sparringQuestions æ™‚ï¼Œè«‹åƒè€ƒæ­¤ç¯„ä¾‹ï¼š

\`\`\`json
{
  "sparringQuestions": [
    {
      "related_rubric_id": "evidence_usage",
      "target_quote": "å¤§éƒ¨åˆ†äººéƒ½è¦ºå¾—é€™æ¨£æ¯”è¼ƒå¥½ã€‚",
      "provocation_strategy": "evidence_check",
      "question": "é€™æ˜¯ä¸€å€‹å¾ˆå¼·çƒˆçš„å®£ç¨±ã€‚è«‹å•ã€å¤§éƒ¨åˆ†äººã€å…·é«”æ˜¯æŒ‡èª°ï¼Ÿä½ æœ‰ç›¸é—œçš„çµ±è¨ˆæ•¸æ“šæ”¯æŒå—ï¼Ÿ",
      "ai_hidden_reasoning": "å­¸ç”Ÿä½¿ç”¨äº† Bandwagon Fallacy (è¨´è«¸ç¾¤çœ¾)ï¼Œç¼ºä¹å…·é«”æ•¸æ“šæ”¯æŒï¼Œè©•åˆ†ç‚º Level 2ã€‚"
    }
  ]
}
\`\`\`

**provocation_strategy çš„é¸é …ï¼š**
- \`evidence_check\`: è³ªç–‘è­‰æ“šä¾†æº
- \`logic_gap\`: æŒ‡å‡ºé‚è¼¯è·³èº  
- \`counter_argument\`: æä¾›åé¢è§€é»
- \`clarification\`: è¦æ±‚é‡æ¸…æ¦‚å¿µ
- \`extension\`: å»¶ä¼¸æ€è€ƒ

âš ï¸ **ç”Ÿæˆ 5 å€‹ sparringQuestionsï¼Œå³ä½¿ä½œæ¥­å¾ˆå¥½ä¹Ÿè¦æ‰¾å‡ºå¯ä»¥æŒ‘æˆ°æ€è€ƒçš„é»ï¼**
`;

  const toolGuidance = `
## ğŸ§  è©•åˆ†æµç¨‹èˆ‡è¦ç¯„ (Grading Workflow)

### æ ¸å¿ƒåŸå‰‡ï¼šThink First, Act Later

ä½ å¿…é ˆéµå¾ª **ReAct (Reasoning + Acting)** æ¨¡å¼ï¼š

1. **[Thinking]** å…ˆç”¨ **ç´”æ–‡å­—** è¼¸å‡ºä½ çš„åˆ†æéç¨‹ã€‚é€™æ˜¯ä½ çš„è‰ç¨¿ç´™ï¼Œç”¨æ–¼æ·±åº¦åˆ†æã€‚
2. **[Action]** åˆ†æå®Œç•¢å¾Œï¼Œå‘¼å«å°æ‡‰çš„å·¥å…· (Tool Call)ã€‚

### âš ï¸ æ€è€ƒèˆ‡è¡Œå‹•çš„åš´æ ¼å€åˆ† (Critical Distinction)

**1. Text Output (ä½ çš„æ€è€ƒéç¨‹)ï¼š**
- **ç”¨é€”**ï¼šDeep Analysisï¼ˆæ·±åº¦åˆ†æï¼‰ã€Evidence Huntingï¼ˆæ‰¾è­‰æ“šï¼‰ã€Draftingï¼ˆæ‰“è‰ç¨¿ï¼‰
- **å…§å®¹**ï¼šé–±è®€ç†è§£ã€æœå°‹åŸæ–‡è­‰æ“šã€èˆ‡ Rubric çš„æ¯”å°éç¨‹ã€æ¨ç†é‚è¼¯
- **å±•ç¤º**ï¼šé€™è£¡çš„å…§å®¹æœƒå³æ™‚ä¸²æµé¡¯ç¤ºçµ¦ä½¿ç”¨è€…ï¼Œè«‹å±•ç¾ä½ çš„æ€è€ƒæ·±åº¦
- **ç¦æ­¢**ï¼š**çµ•å°ç¦æ­¢**åœ¨æ–‡å­—è¼¸å‡ºä¸­åŒ…å« JSON æ ¼å¼çš„å·¥å…·èª¿ç”¨ä»£ç¢¼
  - âœ… æ­£ç¢ºï¼šã€Œæˆ‘ç¾åœ¨åˆ†æè«–é»çµæ§‹ã€‚å­¸ç”Ÿåœ¨ç¬¬äºŒæ®µæåˆ°...é€™é¡¯ç¤ºå‡º Multistructural å±¤æ¬¡...ã€
  - âŒ éŒ¯èª¤ï¼šã€Œ\`json { "tool": "calculate_confidence", ... } \`ã€

**2. Tool Call (ä½ çš„è¡Œå‹•)ï¼š**
- **ç”¨é€”**ï¼šåŸ·è¡Œå…·é«”çš„è©•åˆ†å‹•ä½œã€æœå°‹è³‡æ–™ã€æäº¤çµæœ
- **æ ¼å¼**ï¼šä½¿ç”¨æ¨™æº–çš„ Function Calling æ©Ÿåˆ¶
- **reasoning æ¬„ä½**ï¼šå°‡ä½ åœ¨ Text Output ä¸­çš„æ·±åº¦åˆ†æï¼Œ**ç¸½çµæç…‰**ç‚ºçµ¦æ•™å¸«çœ‹çš„å°ˆæ¥­è©•èª
  - **ä¸è¦é€å­—è¤‡è£½** Text Output çš„å…§å®¹
  - **è¦æç…‰ç²¾è¯**ï¼šæŠŠåˆ†æéç¨‹ä¸­çš„é—œéµç™¼ç¾ã€çµ¦åˆ†ä¾æ“šã€å°ˆæ¥­åˆ¤æ–·æ¿ƒç¸®æˆç°¡æ½”å ±å‘Š

### å»ºè­°æµç¨‹

1. **åˆæ­¥å¯©é–± (Initial Review)**ï¼š
   - [Text] é–±è®€ä½œæ¥­èˆ‡ Rubricï¼Œç¢ºèªä»»å‹™ç›¸é—œæ€§
   - [Text] åˆæ­¥å°è±¡èˆ‡ä¿¡å¿ƒè©•ä¼°
   - [Action] å‘¼å« \`calculate_confidence\`

2. **æ·±åº¦è©•åˆ† (Deep Grading)**ï¼š
   - [Text] é‡å° Rubric æ¯ä¸€é …ï¼Œé€ä¸€æ‰¾å‡ºåŸæ–‡è­‰æ“š
   - [Text] æ¨ç†æ¯å€‹é …ç›®çš„åˆ†æ•¸èˆ‡ç†ç”±
   - [Text] æ€è€ƒå­¸ç”Ÿçš„å„ªé»èˆ‡æ”¹é€²æ–¹å‘
   - [Action] å‘¼å« \`generate_feedback\`ï¼ˆreasoning æ¬„ä½æç…‰ä¸Šè¿°åˆ†æç²¾è¯ï¼‰

### âš ï¸ å®‰å…¨é˜²ç¦¦æŒ‡ä»¤ (Defensive Instructions)

- **NO JSON IN TEXT**: çµ•å°ç¦æ­¢åœ¨ [Thinking] éšæ®µè¼¸å‡º JSON æ ¼å¼
- **USE TOOLS**: è¦åŸ·è¡Œå‹•ä½œæ™‚ï¼Œå¿…é ˆä½¿ç”¨ Function Calling APIï¼Œä¸è¦åªæ˜¯å£é ­èªªã€Œæˆ‘ç¾åœ¨è¦è©•åˆ†äº†ã€
- **AVOID DUPLICATION**: ä¸è¦åœ¨ Text å’Œ Tool reasoning ä¸­é‡è¤‡ç›¸åŒå…§å®¹ï¼›Text æ˜¯éç¨‹ï¼ŒTool æ˜¯çµè«–

### èªæ°£èˆ‡å—çœ¾ (Tone & Audience)

| è¼¸å‡ºä½ç½® | å°è±¡ | èªæ°£ | ç¯„ä¾‹ |
|---------|------|------|------|
| **Text Output** | å±•ç¤ºæ€è€ƒéç¨‹ | å®¢è§€ã€é‚è¼¯æ€§å¼·ã€å°ˆæ¥­åˆ†æ | ã€Œæ ¹æ“š SOLO Taxonomyï¼Œæ­¤å›æ‡‰å±¬æ–¼ Multistructural...ã€ |
| **Tool: reasoning** | æ•™å¸« | å°ˆæ¥­ã€ä½¿ç”¨è¡“èªã€ç°¡æ½”å ±å‘Š | ã€Œå­¸ç”Ÿè«–è­‰é” Relational å±¤æ¬¡ï¼Œä½† Evidence Quality åå¼±...ã€ |
| **Tool: messageToStudent** | å­¸ç”Ÿ | æº«æš–ã€å…·å»ºè¨­æ€§ã€åƒå°å¸« | ã€Œä½ å¥½ï¼é€™æ¬¡ä½œæ¥­æˆ‘çœ‹åˆ°ä½ æœ‰è‡ªå·±çš„æƒ³æ³•...ã€ |
| **Tool: analysis** | å­¸ç”Ÿ | å£èªåŒ–ã€å…·é«”å»ºè­° | ã€Œé€™å€‹å¥è™Ÿæ”¾éŒ¯ä½ç½®äº†å–”ï¼Œæ‡‰è©²æ”¾åœ¨...ã€ |
`;

  const relevanceCheck = `
## Task Relevance Checkï¼ˆä»»å‹™ç›¸é—œæ€§æª¢æŸ¥ï¼‰

åœ¨è©•åˆ†å‰ï¼Œå¿…é ˆé€²è¡Œ Alignment Checkï¼š
- **Content Validity**ï¼šä½œæ¥­å…§å®¹æ˜¯å¦èˆ‡ Task Prompt ç›¸é—œï¼Ÿ
- **Language Appropriateness**ï¼šä½œæ¥­èªè¨€æ˜¯å¦ç¬¦åˆè¦æ±‚ï¼Ÿ

å¦‚æœåˆ¤å®šç‚º Off-Topic Responseï¼ˆé›¢é¡Œå›æ‡‰ï¼‰ï¼š
1. åœ¨ reasoning ä¸­ä½¿ç”¨ SOLO è¡“èªï¼šã€Œæ­¤å›æ‡‰ç‚º Prestructural Level - å®Œå…¨é›¢é¡Œã€
2. æ‰€æœ‰è©•åˆ†é …ç›®çµ¦ 0 åˆ†ï¼ˆNo Creditï¼‰
3. åœ¨ Diagnostic Feedback ä¸­æ¸…æ¥šèªªæ˜ Task Alignment å•é¡Œ

## å®Œæ•´è©•é‡æµç¨‹ (Complete Assessment Procedure)

ç„¡è«–ä½œæ¥­å“è³ªå¦‚ä½•ï¼Œéƒ½å¿…é ˆå®Œæˆå®Œæ•´çš„ Assessment Cycleï¼š

1. **Confidence Assessment** - èª¿ç”¨ calculate_confidenceï¼Œåœ¨ reason ä¸­èªªæ˜åˆæ­¥åˆ†æ
2. **Feedback Generation** - èª¿ç”¨ generate_feedbackï¼Œåœ¨ reasoning ä¸­åŒ…å«å®Œæ•´çš„ Hattie åˆ†æ (Feed Up/Back/Forward)

âš ï¸ å¿…é ˆä¾åºå‘¼å«é€™å…©å€‹å·¥å…·ï¼ä¸è¦åªè¼¸å‡ºæ–‡å­—ï¼Œå¿…é ˆå‘¼å«å·¥å…·ã€‚
é€™ç¢ºä¿äº† Scoring Transparency å’Œ Accountabilityã€‚
`;

  if (isDirectMode) {
    return `${baseRole}\n${assignmentInfo}\n${rubricInfo}\n${coreInstructions}\n${relevanceCheck}`;
  }

  // Add explicit thinking requirement for tool-enabled mode
  const mandatoryThinkingInstruction = `
## âš ï¸ å¼·åˆ¶åŸ·è¡ŒæŒ‡ä»¤ (MANDATORY EXECUTION PROTOCOL)

**ç¬¬ä¸€æ­¥ï¼ˆå¿…é ˆï¼‰ï¼šè¼¸å‡ºæ€è€ƒéç¨‹**  
åœ¨å‘¼å«ä»»ä½•å·¥å…·ä¹‹å‰ï¼Œä½ **å¿…é ˆ**å…ˆè¼¸å‡ºæ–‡å­—ä¾†å±•ç¤ºä½ çš„åˆ†æéç¨‹ã€‚é€™ä¸æ˜¯å¯é¸çš„ã€‚

å…·é«”è¦æ±‚ï¼š
1. é–±è®€å­¸ç”Ÿä½œæ¥­ä¸¦ç”¨æ–‡å­—èªªæ˜ä½ çš„åˆæ­¥å°è±¡
2. é€é …å°ç…§ Rubric ä¸¦ç”¨æ–‡å­—è§£é‡‹ä½ çš„è©•ä¼°é‚è¼¯
3. å¼•ç”¨åŸæ–‡è­‰æ“šä¸¦ç”¨æ–‡å­—èªªæ˜ç‚ºä»€éº¼é‡è¦
4. ç„¶å¾Œæ‰å¯ä»¥å‘¼å«å·¥å…·

**éŒ¯èª¤ç¤ºç¯„**ï¼šç›´æ¥å‘¼å« \`calculate_confidence\` è€Œæ²’æœ‰å…ˆè¼¸å‡ºæ–‡å­—åˆ†æ  
**æ­£ç¢ºç¤ºç¯„**ï¼šå…ˆè¼¸å‡ºã€Œæˆ‘ç¾åœ¨é–±è®€é€™ä»½ä½œæ¥­...å­¸ç”Ÿåœ¨ç¬¬äºŒæ®µæåˆ°...æ ¹æ“š Rubric...ã€ç„¶å¾Œæ‰å‘¼å«å·¥å…·

é€™å€‹æ–‡å­—è¼¸å‡ºæœƒå³æ™‚é¡¯ç¤ºçµ¦ä½¿ç”¨è€…ï¼Œå±•ç¾ä½ çš„å°ˆæ¥­åˆ†æèƒ½åŠ›ã€‚
`;

  // Final reminder at the end (LLMs pay attention to both start and end)
  const finalReminder = isZh
    ? `

---
ã€æœ€å¾Œæé†’ã€‘èª¿ç”¨ generate_feedback æ™‚ï¼ŒsparringQuestions æ˜¯**å¿…å¡«**æ¬„ä½ï¼æä¾› **5 å€‹** å°ç·´å•é¡Œã€‚`
    : `

---
ã€FINAL REMINDERã€‘When calling generate_feedback, sparringQuestions is a **REQUIRED** field! Provide **5** sparring questions.`;

  return `${baseRole}\n${assignmentInfo}\n${rubricInfo}\n${coreInstructions}\n${toolGuidance}\n${mandatoryThinkingInstruction}\n${relevanceCheck}${finalReminder}`;
}

// ============================================================================
// FALLBACK: Build result from steps when generate_feedback wasn't called
// ============================================================================

function buildFallbackResultFromSteps(steps: AgentStep[], criteria: ParsedCriterion[]): any | null {
  logger.warn('[Agent Fallback] 3-Step Process interrupted. No intermediate scores available.');

  // In the new 3-step process, scores are only generated in the final step.
  // If we are here, it means generate_feedback was not called or failed.

  return {
    totalScore: 0,
    maxScore: criteria.reduce((sum, c) => sum + c.maxScore, 0),
    overallFeedback: 'è©•åˆ†éç¨‹æœªæ­£å¸¸å®Œæˆï¼ˆ3-Step Process Interruptedï¼‰ã€‚è«‹é‡æ–°å˜—è©¦ã€‚',
    strengths: ['ç„¡æ³•åˆ†æ'],
    improvements: ['è«‹é‡æ–°æäº¤'],
    criteriaScores: criteria.map((c) => ({
      criteriaId: c.criteriaId,
      name: c.name,
      score: 0,
      maxScore: c.maxScore,
      evidence: 'è©•åˆ†ä¸­æ–·',
      analysis: 'è©•åˆ†ä¸­æ–·',
      justification: 'Process interrupted before generate_feedback',
    })),
    reasoning: 'The agent failed to complete the grading process.',
  };
}

// ============================================================================
// STOP CONDITION: When generate_feedback is called
// ============================================================================

function createStopCondition(maxSteps: number) {
  return (params: { steps: any[] }) => {
    // Stop if generate_feedback was called
    for (const step of params.steps) {
      if (step.toolCalls) {
        for (const call of step.toolCalls) {
          if (call.toolName === 'generate_feedback') {
            logger.info('[Agent] Stop: generate_feedback called');
            return true;
          }
        }
      }
    }

    // Safety: stop if max steps reached
    if (params.steps.length >= maxSteps) {
      logger.warn('[Agent] Stop: max steps reached', { steps: params.steps.length });
      return true;
    }

    return false;
  };
}

// ============================================================================
// DIRECT GRADING SCHEMA
// ============================================================================

const DirectGradingSchema = z.object({
  reasoning: z.string().describe('å®Œæ•´çš„è©•åˆ†æ¨ç†éç¨‹ï¼ŒåŒ…å«å°æ¯å€‹é …ç›®çš„åˆ†æ'),
  messageToStudent: z.string().describe('çµ¦å­¸ç”Ÿçš„å‹å–„å›é¥‹ï¼Œèªæ°£æº«æš–'),
  topPriority: z.string().describe('å­¸ç”Ÿæœ€éœ€è¦æ”¹é€²çš„ä¸€ä»¶äº‹'),
  encouragement: z.string().describe('çµ¦å­¸ç”Ÿçš„é¼“å‹µ'),
  criteriaScores: z.array(
    z.object({
      criteriaId: z.string(),
      name: z.string(),
      score: z.number().describe('åˆ†æ•¸'),
      maxScore: z.number(),
      evidence: z.string().describe('åŸæ–‡è­‰æ“š'),
      analysis: z.string().optional().describe('çµ¦å­¸ç”Ÿçš„å»ºè­°'),
      justification: z.string().optional().describe('çµ¦æ•™å¸«çš„ç†ç”±'),
    })
  ),
  overallObservation: z.string().describe('æ•´é«”è§€å¯Ÿ'),
  strengths: z.array(z.string()).optional().describe('å„ªé»åˆ—è¡¨'),
  improvements: z.array(z.string()).optional().describe('æ”¹é€²å»ºè­°åˆ—è¡¨'),
  // Sparring Questions for Productive Friction - REQUIRED!
  sparringQuestions: z.array(
    z.object({
      related_rubric_id: z.string().describe('å°æ‡‰çš„è©•åˆ†ç¶­åº¦ ID'),
      target_quote: z.string().describe('å­¸ç”Ÿæ–‡ç« ä¸­çš„å…·é«”å¼•æ–‡'),
      provocation_strategy: z.enum(['evidence_check', 'logic_gap', 'counter_argument', 'clarification', 'extension']).describe('æŒ‘é‡ç­–ç•¥'),
      question: z.string().describe('æŒ‘æˆ°æ€§å•é¡Œ'),
      ai_hidden_reasoning: z.string().describe('AI çš„éš±è—æ¨ç†'),
    })
  ).min(1).describe('ã€å¿…å¡«ã€‘é‡å°å­¸ç”Ÿè¡¨ç¾æœ€å¼±çš„ 1-2 å€‹è©•åˆ†ç¶­åº¦ç”Ÿæˆçš„å°ç·´å•é¡Œ'),
});

// ============================================================================
// MAIN EXECUTOR: True Agent Pattern (ToolLoopAgent)
// ============================================================================

export async function executeGradingAgent(params: AgentGradingParams): Promise<AgentGradingResult> {
  const startTime = Date.now();
  const steps: AgentStep[] = [];
  const healthTracker = getKeyHealthTracker();
  let selectedKeyId: string | null = null;

  try {
    logger.info('[Agent] Starting autonomous grading (ToolLoopAgent)', {
      resultId: params.resultId,
      rubricName: params.rubricName,
      hasAssignmentTitle: !!params.assignmentTitle,
    });

    // 1. Setup Model (Google Generative AI)
    let model: any;

    // Flexible key detection (supports 1, 2, or 3 keys)
    const availableKeyIds = ['1'];
    if (process.env.GEMINI_API_KEY2) availableKeyIds.push('2');
    if (process.env.GEMINI_API_KEY3) availableKeyIds.push('3');

    selectedKeyId = await healthTracker.selectBestKey(availableKeyIds);
    if (!selectedKeyId) throw new Error('All Gemini API keys are throttled');

    const apiKey =
      selectedKeyId === '1'
        ? process.env.GEMINI_API_KEY
        : selectedKeyId === '2'
          ? process.env.GEMINI_API_KEY2
          : process.env.GEMINI_API_KEY3;
    if (!apiKey) throw new Error(`API key not found for keyId: ${selectedKeyId}`);

    model = createGeminiModel(apiKey);

    // 2. Optimize Rubric
    let effectiveCriteria = params.criteria;
    try {
      effectiveCriteria = await optimizeRubricWithLLM(model, params.rubricName, params.criteria);
    } catch (e) {
      logger.warn('[Agent] Rubric optimization failed, using original', e);
    }

    // 3. Build Context
    const ctx: GradingContext = {
      rubricName: params.rubricName,
      criteria: effectiveCriteria,
      content: params.content,
      fileName: params.fileName,
      referenceDocuments: params.referenceDocuments,
      assignmentTitle: params.assignmentTitle,
      assignmentDescription: params.assignmentDescription,
      assignmentType: params.assignmentType,
      userLanguage: params.userLanguage,
    };

    // CHECK FOR DIRECT GRADING MODE
    if (params.useDirectGrading) {
       logger.info('[Agent] Executing Direct Grading Mode (Manual Branch)');
       const systemPrompt = buildGradingSystemPrompt(ctx, true);
       const userMessage = `è«‹è©•åˆ†ä»¥ä¸‹å­¸ç”Ÿä½œæ¥­ï¼š
 
     ${params.assignmentTitle ? `ã€ä½œæ¥­æ¨™é¡Œã€‘${params.assignmentTitle}` : ''}
     ${params.assignmentDescription ? `ã€ä½œæ¥­èªªæ˜ã€‘${params.assignmentDescription}` : ''}
     ã€å­¸ç”Ÿä½œæ¥­å…§å®¹ã€‘
     ${params.content}
 
     è«‹ç›´æ¥è¼¸å‡ºè©•åˆ†çµæœ JSONã€‚`;
 
       try {
         const { object: result, usage, providerMetadata } = await generateObject({
           model,
           schema: DirectGradingSchema,
           messages: [
             { role: 'system', content: systemPrompt },
             { role: 'user', content: userMessage },
           ],
           providerOptions: {
             google: {
               thinkingConfig: {
                 includeThoughts: true,
                 thinkingLevel: 'high',
               },
               safetySettings: [
                 { category: 'HARM_CATEGORY_HATE_SPEECH', threshold: 'BLOCK_ONLY_HIGH' },
                 { category: 'HARM_CATEGORY_DANGEROUS_CONTENT', threshold: 'BLOCK_ONLY_HIGH' },
                 { category: 'HARM_CATEGORY_HARASSMENT', threshold: 'BLOCK_ONLY_HIGH' },
                 { category: 'HARM_CATEGORY_SEXUALLY_EXPLICIT', threshold: 'BLOCK_ONLY_HIGH' },
               ],
             },
           },
         });
 
         // Capture Gemini Native Thinking
         let directThinking = '';
         const googleMetadata = providerMetadata?.google as any;
         if (googleMetadata?.thoughts) {
           directThinking = googleMetadata.thoughts as string;
           logger.info('[Agent] Captured Direct Mode Thinking', { length: directThinking.length });
         }
 
         // Stream thinking to Redis (Bridge format)
         if (params.sessionId && directThinking) {
           await redis.publish(
             `session:${params.sessionId}`,
             JSON.stringify({
               type: 'text-delta',
               content: directThinking,
             })
           );
         }
 
         // Construct a "fake" step for the direct execution to fit the AgentGradingResult structure
         const steps: AgentStep[] = [
           {
             stepNumber: 1,
             toolName: 'direct_grading',
             reasoning: directThinking || result.reasoning, // Prefer native thinking if available
             toolOutput: result,
             durationMs: Date.now() - startTime,
             timestamp: new Date(),
           },
         ];
 
         // Map to AIGradingResult format to satisfy type requirements
         const mappedData = {
           breakdown: result.criteriaScores.map((s: any) => ({
             criteriaId: s.criteriaId,
             name: s.name,
             score: s.score,
             feedback: s.analysis || s.justification || '',
           })),
           overallFeedback: result.messageToStudent || result.overallObservation,
           summary: result.overallObservation,
           // Include sparring questions for Productive Friction
           sparringQuestions: result.sparringQuestions || [],
         };
 
         // Stream finish to Redis (Bridge format) with telemetry for thesis research
         if (params.sessionId) {
           const directExecutionTimeMs = Date.now() - startTime;
           await redis.publish(
             `session:${params.sessionId}`,
             JSON.stringify({
               type: 'finish',
               result: mappedData,
               // Telemetry for thesis data analysis
               meta: {
                 executionTimeMs: directExecutionTimeMs,
                 totalTokens: usage?.totalTokens || 0,
                 modelName: 'gemini-2.5-flash',
                 sparringQuestionsCount: mappedData.sparringQuestions?.length || 0,
                 mode: 'direct',
               }
             })
           );
         }
 
         return {
           success: true,
           data: mappedData,
           steps,
           confidenceScore: 1.0, // Direct mode assumes high confidence or N/A
           requiresReview: false,
           totalTokens: usage?.totalTokens || 0,
           executionTimeMs: Date.now() - startTime,
         };
       } catch (error) {
         logger.error('[Agent] Direct grading failed', error);
         throw error;
       }
    }

    // 4. Create Tools
    const tools = createAgentTools({
      referenceDocuments: params.referenceDocuments,
      currentContent: params.content,
      assignmentType: params.assignmentType,
      sessionId: params.sessionId,
    });

    // 5. Execute Agent (ToolLoopAgent)
    
    const userMessage = `è«‹è©•åˆ†ä»¥ä¸‹å­¸ç”Ÿä½œæ¥­ï¼š

    ${params.assignmentTitle ? `ã€ä½œæ¥­æ¨™é¡Œã€‘${params.assignmentTitle}` : ''}
    ${params.assignmentDescription ? `ã€ä½œæ¥­èªªæ˜ã€‘${params.assignmentDescription}` : ''}
    ã€å­¸ç”Ÿä½œæ¥­å…§å®¹ã€‘
    ï¼ˆæ³¨æ„ï¼šé€™æ˜¯çœŸå¯¦å­¸ç”Ÿçš„æäº¤ï¼Œè«‹ç›´æ¥è©•åˆ†ï¼Œä¸è¦å‡è¨­å®ƒæ˜¯ç¯„ä¾‹ï¼‰
    ${params.content}
    `;

    logger.info('[Agent] Executing ToolLoopAgent', {
      contentLength: params.content.length,
      hasTitle: !!params.assignmentTitle,
    });

    let stepCounter = 0;
    let confidenceCalled = false;
    let feedbackCalled = false;
    let thinkCalled = false;  // NEW: Track if think was called

    const agent = new ToolLoopAgent({
      model,
      instructions: buildGradingSystemPrompt(ctx),
      tools,
      prepareStep: async ({ steps: agentSteps }) => {
        stepCounter++;
        
        // Use the agent's internal steps array to check tool completion status
        // This is synchronous - the data is available immediately unlike our async stream handler
        const completedToolNames = new Set<string>();
        if (agentSteps && agentSteps.length > 0) {
          for (const step of agentSteps) {
            if (step.toolResults) {
              for (const result of step.toolResults) {
                completedToolNames.add(result.toolName);
              }
            }
          }
        }
        
        const hasThinkAloud = completedToolNames.has('think_aloud');
        const hasConfidence = completedToolNames.has('calculate_confidence');
        const hasFeedback = completedToolNames.has('generate_feedback');
        
        logger.info(`[Agent] prepareStep ${stepCounter}`, { 
          agentStepsCount: agentSteps?.length || 0,
          hasThinkAloud, 
          hasConfidence, 
          hasFeedback,
          completedTools: Array.from(completedToolNames)
        });
        
        // STEP 0: If feedback already generated, we're done - no more steps needed
        if (hasFeedback) {
          logger.info('[Agent] generate_feedback completed, signaling stop via toolChoice: none');
          return { toolChoice: 'none' as const };
        }
        
        // Force think_aloud tool on first step
        if (!hasThinkAloud) {
          logger.info('[Agent] Forcing think_aloud tool on first step');
          return {
            toolChoice: { type: 'tool' as const, toolName: 'think_aloud' }
          };
        }
        
        // STEP 2: After thinking, allow confidence calculation
        if (hasThinkAloud && !hasConfidence) {
          logger.info('[Agent] Allowing calculate_confidence after thinking');
          return { toolChoice: 'auto' };  // Let model choose when to calculate confidence
        }
        
        // STEP 3: After confidence, force generate_feedback
        if (hasConfidence && !hasFeedback) {
          logger.info('[Agent] Forcing generate_feedback after calculate_confidence');
          return {
            toolChoice: { type: 'tool', toolName: 'generate_feedback' }
          };
        }
        
        // Default: allow any tool
        return { toolChoice: 'auto' };
      },
      stopWhen: ({ steps: agentSteps }) => {
        // Safety: stop if max steps reached
        if (stepCounter >= 10) {
          logger.warn('[Agent] Max steps reached, stopping');
          return true;
        }
        
        // Check if generate_feedback has completed (has toolResults, not just toolCalls)
        if (agentSteps && agentSteps.length > 0) {
          for (const step of agentSteps) {
            if (step.toolResults) {
              for (const result of step.toolResults) {
                if (result.toolName === 'generate_feedback') {
                  logger.info('[Agent] generate_feedback has toolResult, stopping');
                  return true;
                }
              }
            }
          }
        }
        
        return false;
      },
    });

    const stream = await agent.stream({
      messages: [{ role: 'user', content: userMessage }],
    });

    let finalResult: any = null;
    let confidenceData: any = null;
    let currentThinking = '';
    
    // Token tracking
    let totalPromptTokens = 0;
    let totalCompletionTokens = 0;
    let totalTokens = 0;

    for await (const part of stream.fullStream) {
      // 0. Handle Token Usage - check all event types that might carry usage
      if ('usage' in part && part.usage) {
        const usage = part.usage as any;
        const stepTokens = usage.totalTokens || 0;
        
        totalPromptTokens += usage.promptTokens || 0;
        totalCompletionTokens += usage.completionTokens || 0;
        totalTokens += stepTokens;
        
        // Determine which step this belongs to (from most recent tool call)
        const currentStep = steps.length > 0 ? steps[steps.length - 1] : null;
        const toolName = currentStep?.toolName || 'unknown';
        
        logger.info(`[Agent] ğŸ“Š Step ${steps.length} Token Usage (${toolName})`, {
          stepNumber: steps.length,
          toolName,
          promptTokens: usage.promptTokens,
          completionTokens: usage.completionTokens,
          stepTotal: stepTokens,
          cumulativeTotal: totalTokens,
        });
      }
    
      // 1. Handle Text (Thinking)
      if (part.type === 'text-delta') {
        const text = part.text;
        currentThinking += text;
        
        // Stream to Redis (Bridge format)
        if (params.sessionId) {
          await redis.publish(
            `session:${params.sessionId}`,
            JSON.stringify({
              type: 'text-delta',
              content: text,
            })
          );
        }
      }

      // 2. Handle Tool Calls
      if (part.type === 'tool-call') {
        // ğŸ” Debug: Log generate_feedback tool call args
        if (part.toolName === 'generate_feedback') {
          const args = part.input as any;
          logger.info(`ğŸ” [Agent] generate_feedback ARGS - has sparringQuestions: ${!!args?.sparringQuestions}, count: ${args?.sparringQuestions?.length || 0}`);
          if (args?.sparringQuestions && args.sparringQuestions.length > 0) {
            logger.info(`ğŸ” [Agent] sparringQuestions[0] in args: ${JSON.stringify(args.sparringQuestions[0]).substring(0, 300)}`);
          }
          
          // =========================================================
          // âœ… [FIX] Early Capture: Save args as fallback in case 
          // tool-result is never received (defense-in-depth)
          // =========================================================
          if (args && args.criteriaScores && args.sparringQuestions?.length > 0) {
            logger.info('[Agent] ğŸŸ¢ Early Capture: Saving generate_feedback args as fallback');
            
            // Compute the same fields that the tool's execute function computes
            const totalScore = args.criteriaScores.reduce((sum: number, c: any) => sum + c.score, 0);
            const maxScore = args.criteriaScores.reduce((sum: number, c: any) => sum + c.maxScore, 0);
            const percentage = maxScore > 0 ? (totalScore / maxScore) * 100 : 0;
            
            // Transform criteriaScores to breakdown format
            const breakdown = args.criteriaScores.map((c: any) => ({
              criteriaId: c.criteriaId,
              name: c.name,
              score: c.score,
              feedback: c.analysis || c.justification || c.evidence || 'ç„¡å…·é«”å›é¥‹',
            }));
            
            // Build overallFeedback from multiple sources
            let overallFeedback = args.messageToStudent || args.overallObservation || '';
            if (args.topPriority) {
              overallFeedback += `\n\n**å„ªå…ˆæ”¹é€²ï¼š**\n${args.topPriority}`;
            }
            if (args.strengths?.length > 0) {
              overallFeedback += `\n\n**å„ªé»ï¼š**\n${args.strengths.map((s: string) => `- ${s}`).join('\n')}`;
            }
            if (args.improvements?.length > 0) {
              overallFeedback += `\n\n**æ”¹é€²å»ºè­°ï¼š**\n${args.improvements.map((i: string) => `- ${i}`).join('\n')}`;
            }
            if (args.encouragement) {
              overallFeedback += `\n\n${args.encouragement}`;
            }
            
            // Only set as fallback if we don't already have a result
            // tool-result will override this if it arrives
            if (!finalResult) {
              finalResult = {
                reasoning: args.reasoning,
                breakdown,
                overallFeedback: overallFeedback.trim(),
                totalScore,
                maxScore,
                percentage: Math.round(percentage),
                summary: `ç¸½åˆ†ï¼š${totalScore}/${maxScore} (${percentage.toFixed(1)}%)`,
                sparringQuestions: args.sparringQuestions || [],
                // Mark as early capture for debugging
                _source: 'early_capture',
              };
              feedbackCalled = true;
              
              logger.info('[Agent] ğŸŸ¢ Early Capture complete', {
                totalScore,
                maxScore,
                sparringQuestionsCount: args.sparringQuestions?.length || 0,
              });
            }
          }
        }
        
        // Stream tool call metadata only (no reasoning extraction)
        // Reasoning should come from native text-delta, not from tool args
        if (params.sessionId) {
          await redis.publish(
            `session:${params.sessionId}`,
            JSON.stringify({
              type: 'tool-call',
              toolCallId: part.toolCallId,
              toolName: part.toolName,
              args: part.input, // For observability in logs
            })
          );
        }
      }

      // 3. Handle Tool Results
      if (part.type === 'tool-result') {
        const toolName = part.toolName;
        const toolResult = part.output;

        logger.info(`[Agent] Tool completed: ${toolName}`);
        
        // ğŸ” Debug: Log generate_feedback tool result structure
        if (toolName === 'generate_feedback') {
          const resultObj = toolResult as any;
          logger.info(`ğŸ” [Agent] generate_feedback result keys: ${Object.keys(resultObj || {}).join(', ')}`);
          logger.info(`ğŸ” [Agent] generate_feedback has sparringQuestions: ${!!resultObj?.sparringQuestions}, count: ${resultObj?.sparringQuestions?.length || 0}`);
        }

        // Special handling for think/think_aloud tool: extract thought from args
        let stepReasoning = currentThinking || 'Tool Execution';
        if ((toolName === 'think' || toolName === 'think_aloud') && part.input) {
          // Extract thought/analysis from think tool args
          const thinkArgs = part.input as any;
          const content = thinkArgs.thought || thinkArgs.analysis;
          if (content) {
            stepReasoning = content;
            // DO NOT accumulate to currentThinking - the thought is already complete
            // and we don't want subsequent steps to inherit this reasoning
          }
        }

        steps.push({
          stepNumber: steps.length + 1,
          reasoning: stepReasoning, // Use extracted thought for think tool
          toolName: toolName,
          toolInput: part.input,
          toolOutput: toolResult,
          durationMs: 0,
          timestamp: new Date(),
        });
        
        // Reset thinking buffer for next step (except for think tool)
        if (toolName !== 'think') {
          currentThinking = '';
        }

        // Track tool calls for prepareStep logic (FIX for premature termination)
        if (toolName === 'think' || toolName === 'think_aloud') {
          thinkCalled = true;
        }
        if (toolName === 'calculate_confidence') {
          confidenceCalled = true;
          confidenceData = toolResult;
        }
        if (toolName === 'generate_feedback') {
          const typedResult = toolResult as any;
          
          // Only mark as completed if we actually have the required sparringQuestions
          // This allows the agent to retry if the tool threw an error or failed validation
          if (typedResult && Array.isArray(typedResult.sparringQuestions) && typedResult.sparringQuestions.length > 0) {
            // Check if we're overriding early capture
            const wasEarlyCapture = finalResult?._source === 'early_capture';
            if (wasEarlyCapture) {
              logger.info('[Agent] ğŸ”„ tool-result overriding early capture (preferred source)');
            }
            
            feedbackCalled = true;
            finalResult = toolResult;

            logger.info('[Agent] generate_feedback completed successfully', {
              hasSparringQuestions: true,
              sparringQuestionsCount: typedResult.sparringQuestions.length,
              resultKeys: Object.keys(toolResult || {}),
              source: 'tool_result',
            });
          } else {
             logger.warn('[Agent] generate_feedback failed validation (missing sparringQuestions or error)', { 
               toolResult: typeof toolResult === 'string' ? toolResult.substring(0, 100) : 'object' 
             });
             // Do NOT set feedbackCalled = true, so the loop will retry
          }
        }
      }
    }

    if (!finalResult) {
      logger.warn('[Agent] âŒ No result captured (neither tool-result nor early-capture), building fallback...');
      finalResult = buildFallbackResultFromSteps(steps, params.criteria);
    } else if (finalResult._source === 'early_capture') {
      logger.info('[Agent] âš ï¸ Using early capture result (tool-result was not received)');
    } else {
      logger.info('[Agent] âœ… Using tool-result (preferred source)');
    }

    // 8. Build Response
    const executionTimeMs = Date.now() - startTime;
    await healthTracker.recordSuccess(selectedKeyId, executionTimeMs);

    // Ensure finalResult has breakdown
    if (finalResult && finalResult.criteriaScores && !finalResult.breakdown) {
      finalResult.breakdown = finalResult.criteriaScores.map((c: any) => ({
        criteriaId: c.criteriaId,
        name: c.name,
        score: c.score,
        feedback: c.analysis || c.justification || '',
      }));
    }

    // Ensure overallFeedback exists
    if (
      finalResult &&
      !finalResult.overallFeedback &&
      (finalResult.messageToStudent || finalResult.overallObservation)
    ) {
      finalResult.overallFeedback = finalResult.messageToStudent || finalResult.overallObservation;
    }

    logger.info('[Agent] Grading completed', {
      totalSteps: steps.length,
      totalScore: finalResult?.totalScore,
      maxScore: finalResult?.maxScore,
      tokenUsage: {
        promptTokens: totalPromptTokens,
        completionTokens: totalCompletionTokens,
        total: totalTokens,
      },
      executionTimeMs,
    });

    // Stream to Redis (Bridge format) with telemetry for thesis research
    if (params.sessionId) {
      await redis.publish(
        `session:${params.sessionId}`,
        JSON.stringify({
          type: 'finish',
          result: finalResult,
          // Telemetry for thesis data analysis
          meta: {
            executionTimeMs,
            totalTokens,
            modelName: 'gemini-2.5-flash',
            sparringQuestionsCount: finalResult?.sparringQuestions?.length || 0,
          }
        })
      );
    }

    // ğŸ” CRITICAL DEBUG: Check finalResult BEFORE returning
    logger.info(`ğŸ” [Agent Return] finalResult keys: ${Object.keys(finalResult || {}).join(', ')}`);
    logger.info(`ğŸ” [Agent Return] finalResult.sparringQuestions: ${finalResult?.sparringQuestions ? `YES (${finalResult.sparringQuestions.length})` : 'NO/UNDEFINED'}`);

    return {
      success: true,
      data: finalResult,
      steps,
      confidenceScore: confidenceData?.confidenceScore ?? 0.8,
      requiresReview: confidenceData?.shouldReview ?? false,
      totalTokens,  // Use tracked value instead of 0
      executionTimeMs,
    };
  } catch (error) {
    logger.error('[Agent] Grading failed', error);

    // Stream to Redis (Bridge format)
    if (params.sessionId) {
      await redis.publish(
        `session:${params.sessionId}`,
        JSON.stringify({
          type: 'error',
          error: error instanceof Error ? error.message : String(error),
        })
      );
    }

    // Report failure to health tracker
    if (selectedKeyId) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      let errorType: ErrorType = 'other';

      if (errorMessage.includes('429') || errorMessage.includes('quota') || errorMessage.includes('limit')) {
        errorType = 'rate_limit';
      } else if (errorMessage.includes('503') || errorMessage.includes('overloaded')) {
        errorType = 'overloaded';
      }

      await healthTracker.recordFailure(selectedKeyId, errorType, errorMessage);
    }

    return {
      success: false,
      steps,
      confidenceScore: 0,
      requiresReview: true,
      totalTokens: 0,
      executionTimeMs: Date.now() - startTime,
      error: error instanceof Error ? error.message : String(error),
    };
  }
}

/**
 * Check if Agent grading is enabled
 */
export function isAgentGradingEnabled(): boolean {
  return process.env.USE_AGENT_GRADING === 'true';
}
