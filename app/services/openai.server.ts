import OpenAI from 'openai';
import logger from '@/utils/logger';

// OpenAI è©•åˆ†è«‹æ±‚ä»‹é¢ - æ–‡å­—å…§å®¹æ–¹å¼
export interface OpenAIGradingRequest {
    content: string;
    criteria: any[];
    fileName: string;
    rubricName: string;
}

// OpenAI æª”æ¡ˆè©•åˆ†è«‹æ±‚ä»‹é¢ - ç›´æ¥æª”æ¡ˆä¸Šå‚³æ–¹å¼
export interface OpenAIFileGradingRequest {
    fileBuffer: Buffer;
    mimeType: string;
    criteria: any[];
    fileName: string;
    rubricName: string;
}

// OpenAI è©•åˆ†å›æ‡‰ä»‹é¢
export interface OpenAIGradingResponse {
    success: boolean;
    result?: GradingResultData;
    error?: string;
    metadata?: {
        model: string;
        tokens: number;
        duration: number;
        errorType?: string;
        retryable?: boolean;
        fileProcessed?: boolean;
        assistantId?: string;
        threadId?: string;
    };
}

// è©•åˆ†çµæœè³‡æ–™çµæ§‹ï¼ˆèˆ‡ Gemini ä¸€è‡´ï¼‰
export interface GradingResultData {
    totalScore: number;
    maxScore: number;
    breakdown: Array<{
        criteriaId: string;
        score: number;
        feedback: string;
    }>;
    overallFeedback: string;
}

class OpenAIService {
    private client: OpenAI;
    private model: string;
    private requestCount: number = 0;
    private lastRequestTime: number = 0;
    private consecutiveErrors: number = 0;
    private lastErrorTime: number = 0;
    private readonly MIN_REQUEST_INTERVAL = 1000; // 1 ç§’é–“éš”
    private readonly OVERLOAD_BACKOFF_TIME = 30000; // 30 ç§’é€€é¿æ™‚é–“
    private readonly MAX_CONSECUTIVE_ERRORS = 3;

    constructor() {
        const apiKey = process.env.OPENAI_API_KEY;
        
        if (!apiKey) {
            throw new Error('OPENAI_API_KEY environment variable is required');
        }

        this.client = new OpenAI({ apiKey });
        this.model = "gpt-4o-mini"; // ä½¿ç”¨å¤šæ¨¡æ…‹æ¨¡å‹
        
        logger.info(`ğŸ¤– Initialized OpenAIService with model: ${this.model}`);
    }

    /**
     * æ™ºèƒ½ rate limiting
     */
    private async enforceRateLimit(): Promise<void> {
        const now = Date.now();
        
        // æª¢æŸ¥æ˜¯å¦åœ¨éè¼‰æ¢å¾©æœŸ
        if (this.consecutiveErrors >= this.MAX_CONSECUTIVE_ERRORS) {
            const timeSinceLastError = now - this.lastErrorTime;
            if (timeSinceLastError < this.OVERLOAD_BACKOFF_TIME) {
                const remainingWait = this.OVERLOAD_BACKOFF_TIME - timeSinceLastError;
                logger.warn(`ğŸš« OpenAI overload detected, waiting ${Math.round(remainingWait/1000)}s before retry...`);
                await this.sleep(remainingWait);
                this.consecutiveErrors = 0;
            }
        }
        
        const timeSinceLastRequest = now - this.lastRequestTime;
        
        // åŸºæœ¬é–“éš”æ§åˆ¶
        if (timeSinceLastRequest < this.MIN_REQUEST_INTERVAL) {
            const delay = this.MIN_REQUEST_INTERVAL - timeSinceLastRequest;
            logger.info(`â³ OpenAI rate limiting: waiting ${delay}ms`);
            await this.sleep(delay);
        }
        
        this.lastRequestTime = Date.now();
        this.requestCount++;
    }

    /**
     * ä¸»è¦è©•åˆ†å‡½å¼ - æ–‡å­—å…§å®¹è©•åˆ†
     */
    async gradeDocument(request: OpenAIGradingRequest): Promise<OpenAIGradingResponse> {
        const startTime = Date.now();

        try {
            await this.enforceRateLimit();
            
            logger.info(`Starting OpenAI grading for file: ${request.fileName}`);

            // ç”Ÿæˆè©•åˆ†æç¤º
            const prompt = this.generateTextGradingPrompt(request);

            // èª¿ç”¨ OpenAI API
            const response = await this.retryWithBackoff(async () => {
                return await this.client.chat.completions.create({
                    model: this.model,
                    messages: [
                        {
                            role: "system",
                            content: this.generateSystemInstruction()
                        },
                        {
                            role: "user",
                            content: prompt
                        }
                    ],
                    max_tokens: 4000,
                    temperature: 0.1,
                    response_format: { type: "json_object" }
                });
            }, 3, 2000);

            if (!response.choices[0]?.message?.content) {
                throw new Error('OpenAI API returned empty response');
            }

            // è¨˜éŒ„æˆåŠŸ
            this.recordApiSuccess();

            // è§£æå›æ‡‰
            const gradingResult = this.parseGradingResponse(response.choices[0].message.content, request.criteria);
            const duration = Date.now() - startTime;

            logger.info(`OpenAI grading completed for ${request.fileName} in ${duration}ms`);

            return {
                success: true,
                result: gradingResult,
                metadata: {
                    model: this.model,
                    tokens: response.usage?.total_tokens || 0,
                    duration
                }
            };

        } catch (error) {
            const duration = Date.now() - startTime;
            
            // è¨˜éŒ„éŒ¯èª¤
            this.recordApiError(error);
            
            const errorInfo = this.analyzeError(error);
            logger.error(`OpenAI grading failed for ${request.fileName}:`, error);

            return {
                success: false,
                error: errorInfo.userMessage,
                metadata: {
                    model: this.model,
                    tokens: 0,
                    duration,
                    errorType: errorInfo.type,
                    retryable: errorInfo.retryable
                }
            };
        }
    }

    /**
     * ä¸»è¦è©•åˆ†å‡½å¼ - æª”æ¡ˆä¸Šå‚³è©•åˆ†ï¼ˆä½¿ç”¨ Assistants APIï¼‰
     */
    async gradeDocumentWithFile(request: OpenAIFileGradingRequest): Promise<OpenAIGradingResponse> {
        const startTime = Date.now();
        let uploadedFile: OpenAI.Files.FileObject | null = null;
        let assistant: OpenAI.Beta.Assistants.Assistant | null = null;
        let thread: OpenAI.Beta.Threads.Thread | null = null;

        try {
            await this.enforceRateLimit();
            
            logger.info(`Starting OpenAI file grading for: ${request.fileName}`);

            // éšæ®µ1ï¼šä¸Šå‚³æ–‡ä»¶
            uploadedFile = await this.retryWithBackoff(async () => {
                const blob = new Blob([request.fileBuffer], { type: request.mimeType });
                const file = new File([blob], request.fileName, { type: request.mimeType });
                
                return await this.client.files.create({
                    file: file,
                    purpose: 'assistants'
                });
            }, 2, 1000);

            logger.info(`ğŸ“ File uploaded successfully: ${uploadedFile.id}`);

            // éšæ®µ2ï¼šå‰µå»ºå°ˆç”¨ Assistant
            assistant = await this.retryWithBackoff(async () => {
                return await this.client.beta.assistants.create({
                    name: "Document Grader",
                    model: this.model,
                    instructions: this.generateAssistantInstructions(request),
                    tools: [{ type: "file_search" }]
                });
            }, 2, 1000);

            logger.info(`ğŸ¤– Assistant created: ${assistant.id}`);

            // éšæ®µ3ï¼šå‰µå»º Thread ä¸¦ç™¼é€è©•åˆ†è«‹æ±‚
            thread = await this.client.beta.threads.create();

            const message = await this.client.beta.threads.messages.create(thread.id, {
                role: "user",
                content: this.generateFileGradingPrompt(request),
                attachments: [
                    {
                        file_id: uploadedFile.id,
                        tools: [{ type: "file_search" }]
                    }
                ]
            });

            // éšæ®µ4ï¼šé‹è¡Œ Assistant
            const run = await this.client.beta.threads.runs.create(thread.id, {
                assistant_id: assistant.id
            });

            // éšæ®µ5ï¼šç­‰å¾…å®Œæˆ
            const response = await this.waitForRunCompletion(thread.id, run.id);

            if (!response) {
                throw new Error('OpenAI Assistant returned empty response');
            }

            // è¨˜éŒ„æˆåŠŸ
            this.recordApiSuccess();

            // è§£æå›æ‡‰
            const gradingResult = this.parseGradingResponse(response, request.criteria);
            const duration = Date.now() - startTime;

            // éšæ®µ6ï¼šæ¸…ç†è³‡æº
            await this.cleanupResources(uploadedFile, assistant, thread);

            logger.info(`OpenAI file grading completed for ${request.fileName} in ${duration}ms`);

            return {
                success: true,
                result: gradingResult,
                metadata: {
                    model: this.model,
                    tokens: this.estimateTokens(response),
                    duration,
                    assistantId: assistant.id,
                    threadId: thread.id
                }
            };

        } catch (error) {
            const duration = Date.now() - startTime;
            
            // è¨˜éŒ„éŒ¯èª¤
            this.recordApiError(error);
            
            // ç·Šæ€¥æ¸…ç†è³‡æº
            if (uploadedFile || assistant || thread) {
                await this.emergencyCleanup(uploadedFile, assistant, thread);
            }
            
            const errorInfo = this.analyzeError(error);
            logger.error(`OpenAI file grading failed for ${request.fileName}:`, error);

            return {
                success: false,
                error: errorInfo.userMessage,
                metadata: {
                    model: this.model,
                    tokens: 0,
                    duration,
                    errorType: errorInfo.type,
                    retryable: errorInfo.retryable,
                    fileProcessed: !!uploadedFile
                }
            };
        }
    }

    /**
     * ç”Ÿæˆæ–‡å­—è©•åˆ†æç¤º
     */
    private generateTextGradingPrompt(request: OpenAIGradingRequest): string {
        const criteriaText = this.formatCriteriaDescription(request.criteria);

        return `è«‹æ ¹æ“šä»¥ä¸‹è©•åˆ†æ¨™æº–ç‚ºæ–‡ä»¶"${request.fileName}"é€²è¡Œè©³ç´°è©•åˆ†ï¼š

è©•åˆ†æ¨™æº–ã€Œ${request.rubricName}ã€ï¼š
${criteriaText}

## è©•åˆ†è¦æ±‚
è«‹å°æ¯å€‹è©•åˆ†é …ç›®æä¾›ï¼š
1. **å¼•ç”¨åˆ†æ** - å¿…é ˆå¼•ç”¨å…·é«”çš„åŸæ–‡å…§å®¹ä½œç‚ºè­‰æ“š
2. **å„ªé»åˆ†æ** - æŒ‡å‡ºè¡¨ç¾å¥½çš„åœ°æ–¹åŠåŸå› 
3. **æ”¹é€²å»ºè­°** - æä¾›å…·é«”å¯åŸ·è¡Œçš„æ”¹é€²æ–¹å‘
4. **è©•åˆ†ç†ç”±** - èªªæ˜ç‚ºä»€éº¼çµ¦é€™å€‹åˆ†æ•¸

**å¼•ç”¨æ ¼å¼**ï¼šç”¨ã€ŒåŸæ–‡å…§å®¹ã€æ¨™ç¤ºå¼•ç”¨éƒ¨åˆ†

## æ–‡ä»¶å…§å®¹
${request.content}

è«‹ä»¥JSONæ ¼å¼å›æ‡‰ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{
  "totalScore": ç¸½åˆ†æ•¸å­—,
  "maxScore": æ»¿åˆ†æ•¸å­—,
  "breakdown": [
    {
      "criteriaId": "ä½¿ç”¨ä¸Šæ–¹åˆ—å‡ºçš„çœŸå¯¦ID",
      "score": å¾—åˆ†,
      "feedback": "åŸºæ–¼ã€ŒåŸæ–‡å¼•ç”¨ã€çš„è©³ç´°åˆ†æï¼ŒåŒ…æ‹¬ï¼š\\n\\n**è¡¨ç¾å„ªé»ï¼š** å¼•ç”¨åŸæ–‡èªªæ˜å¥½çš„åœ°æ–¹\\n\\n**éœ€è¦æ”¹é€²ï¼š** å¼•ç”¨åŸæ–‡æŒ‡å‡ºå•é¡Œ\\n\\n**æ”¹é€²å»ºè­°ï¼š** å…·é«”å¯åŸ·è¡Œçš„å»ºè­°\\n\\n**è©•åˆ†ç†ç”±ï¼š** ç‚ºä»€éº¼çµ¦é€™å€‹åˆ†æ•¸"
    }
  ],
  "overallFeedback": "æ•´é«”è©•åƒ¹ï¼ŒåŒ…å«æœ€é‡è¦çš„å„ªé»å’Œæ”¹é€²å»ºè­°ï¼Œè¦å¼•ç”¨åŸæ–‡æ”¯æŒ"
}`;
    }

    /**
     * ç”Ÿæˆæª”æ¡ˆè©•åˆ†æç¤º
     */
    private generateFileGradingPrompt(request: OpenAIFileGradingRequest): string {
        const criteriaText = this.formatCriteriaDescription(request.criteria);

        return `è«‹ä»”ç´°åˆ†æä¸Šå‚³çš„æ–‡ä»¶ä¸¦æ ¹æ“šä»¥ä¸‹è©•åˆ†æ¨™æº–é€²è¡Œè©³ç´°è©•åˆ†ï¼š

æª”æ¡ˆåç¨±ï¼š${request.fileName}
è©•åˆ†æ¨™æº–ã€Œ${request.rubricName}ã€ï¼š
${criteriaText}

## è©•åˆ†è¦æ±‚
è«‹è©³ç´°é–±è®€æ–‡ä»¶å…§å®¹ï¼Œé‡å°æ¯å€‹è©•åˆ†é …ç›®æä¾›ï¼š
1. **ç²¾ç¢ºå¼•ç”¨** - å¿…é ˆå¼•ç”¨æ–‡ä»¶ä¸­çš„å…·é«”å…§å®¹ä½œç‚ºåˆ†æä¾æ“š
2. **è­‰æ“šåˆ†æ** - èªªæ˜å¼•ç”¨å…§å®¹ç‚ºä»€éº¼è¡¨ç¾å¥½æˆ–éœ€è¦æ”¹é€²
3. **å…·é«”å»ºè­°** - æä¾›å¯åŸ·è¡Œçš„æ”¹é€²æ–¹å‘
4. **è©•åˆ†èªªæ˜** - è§£é‡‹ç‚ºä»€éº¼çµ¦é€™å€‹åˆ†æ•¸

**é‡è¦æŒ‡å¼•ï¼š**
- æ¯é …åˆ†æéƒ½è¦å¼•ç”¨åŸæ–‡ï¼Œç”¨ã€ŒåŸæ–‡å…§å®¹ã€æ ¼å¼æ¨™ç¤º
- å„ªé»åˆ†æè¦æŒ‡å‡ºå…·é«”å¥½åœ¨å“ªè£¡
- æ”¹é€²å»ºè­°è¦å…·é«”å¯è¡Œï¼Œä¸è¦ç©ºæ³›
- å¦‚æœç„¡æ³•å¾æ–‡ä»¶ä¸­æ‰¾åˆ°ç›¸é—œå…§å®¹ï¼Œè«‹æ˜ç¢ºèªªæ˜

è«‹ä»¥JSONæ ¼å¼å›æ‡‰ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{
  "totalScore": ç¸½åˆ†æ•¸å­—,
  "maxScore": æ»¿åˆ†æ•¸å­—,
  "breakdown": [
    {
      "criteriaId": "ä½¿ç”¨ä¸Šæ–¹åˆ—å‡ºçš„çœŸå¯¦ID",
      "score": å¾—åˆ†,
      "feedback": "è©³ç´°åˆ†æå…§å®¹ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\\n\\n**å¼•ç”¨èˆ‡åˆ†æï¼š** ã€Œæ–‡ä»¶ä¸­çš„å…·é«”å…§å®¹ã€- é€™éƒ¨åˆ†è¡¨ç¾å¦‚ä½•\\n\\n**è¡¨ç¾å„ªé»ï¼š** å¥½çš„åœ°æ–¹åŠåŸå› \\n\\n**æ”¹é€²ç©ºé–“ï¼š** éœ€è¦æ”¹å–„çš„åœ°æ–¹\\n\\n**å…·é«”å»ºè­°ï¼š** å¦‚ä½•æ”¹é€²ï¼Œè¦å¯åŸ·è¡Œ\\n\\n**è©•åˆ†ä¾æ“šï¼š** ç‚ºä»€éº¼çµ¦é€™å€‹åˆ†æ•¸"
    }
  ],
  "overallFeedback": "æ•´é«”è©•åƒ¹å’Œå»ºè­°ï¼Œè¦åŒ…å«ï¼š\\n1. æœ€çªå‡ºçš„å„ªé»ï¼ˆå¼•ç”¨æ”¯æŒï¼‰\\n2. æœ€éœ€è¦æ”¹é€²çš„åœ°æ–¹ï¼ˆå¼•ç”¨æ”¯æŒï¼‰\\n3. å…·é«”çš„ä¸‹ä¸€æ­¥å»ºè­°"
}

å¦‚æœç„¡æ³•å¾æ–‡ä»¶ä¸­æå–ç›¸é—œè³‡è¨Šï¼Œè«‹èªªæ˜åŸå› ã€‚`;
    }

    /**
     * æ ¼å¼åŒ–è©•åˆ†æ¨™æº–æè¿°
     */
    private formatCriteriaDescription(criteria: any[]): string {
        const criteriaList = criteria.map((criterion, index) => {
            const levelsText = criterion.levels 
                ? criterion.levels.map((level: any) => `${level.score}åˆ† - ${level.description}`).join('ï¼›')
                : '';
            
            return `${index + 1}. **${criterion.name}** (${criterion.maxScore}åˆ†)
   ID: "${criterion.id}" â† è«‹åœ¨ JSON ä¸­ä½¿ç”¨æ­¤ ID
   èªªæ˜ï¼š${criterion.description || ''}
   ${levelsText ? `è©•åˆ†ç­‰ç´šï¼š${levelsText}` : ''}`;
        }).join('\n\n');

        const criteriaIds = criteria.map(c => `"${c.id}"`).join(', ');
        
        return `${criteriaList}

**é‡è¦ï¼š** åœ¨ JSON å›æ‡‰ä¸­ï¼Œ"criteriaId" å¿…é ˆå®Œå…¨åŒ¹é…ä¸Šè¿° IDï¼š${criteriaIds}`;
    }

    /**
     * ç”Ÿæˆç³»çµ±æŒ‡ä»¤
     */
    private generateSystemInstruction(): string {
        return `ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡ä»¶è©•åˆ†å°ˆå®¶ã€‚è«‹éµå¾ªä»¥ä¸‹åŸå‰‡ï¼š

1. å®¢è§€å…¬æ­£åœ°è©•åˆ†ï¼ŒåŸºæ–¼æ˜ç¢ºçš„è­‰æ“š
2. æä¾›å»ºè¨­æ€§çš„å›é¥‹å’Œå…·é«”æ”¹é€²å»ºè­°
3. ä½¿ç”¨æ­£é¢é¼“å‹µçš„èªè¨€ï¼Œä½†æŒ‡å‡ºéœ€è¦æ”¹é€²çš„åœ°æ–¹
4. ç¢ºä¿è©•åˆ†èˆ‡æ¨™æº–ä¸€è‡´
5. å›æ‡‰å¿…é ˆæ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼
6. å¦‚æœæ–‡ä»¶å…§å®¹ä¸è¶³ä»¥è©•åˆ†ï¼Œè«‹èªªæ˜åŸå› `;
    }

    /**
     * ç”Ÿæˆ Assistant æŒ‡ä»¤
     */
    private generateAssistantInstructions(request: OpenAIFileGradingRequest): string {
        return `ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡ä»¶è©•åˆ†åŠ©æ‰‹ã€‚ä½ çš„ä»»å‹™æ˜¯ï¼š

1. ä»”ç´°åˆ†æç”¨æˆ¶ä¸Šå‚³çš„æ–‡ä»¶
2. æ ¹æ“šæä¾›çš„è©•åˆ†æ¨™æº–é€²è¡Œå®¢è§€è©•åˆ†
3. ç‚ºæ¯å€‹è©•åˆ†é …ç›®æä¾›è©³ç´°å›é¥‹
4. çµ¦å‡ºæ•´é«”è©•åƒ¹å’Œæ”¹é€²å»ºè­°
5. ä»¥JSONæ ¼å¼å›æ‡‰

æª”æ¡ˆé¡å‹ï¼š${request.mimeType}
è©•åˆ†æ¨™æº–ï¼š${request.rubricName}

è«‹ç¢ºä¿ï¼š
- è©•åˆ†åŸºæ–¼æ–‡ä»¶å¯¦éš›å…§å®¹
- å›é¥‹å…·é«”ä¸”æœ‰å»ºè¨­æ€§
- JSONæ ¼å¼æ­£ç¢ºä¸”å®Œæ•´
- å¦‚æœæ–‡ä»¶ç„¡æ³•è®€å–ï¼Œè«‹èªªæ˜åŸå› `;
    }

    /**
     * ç­‰å¾… Assistant é‹è¡Œå®Œæˆ
     */
    private async waitForRunCompletion(threadId: string, runId: string, maxWaitTime: number = 300000): Promise<string | null> {
        const startTime = Date.now();
        
        while (Date.now() - startTime < maxWaitTime) {
            const run = await this.client.beta.threads.runs.retrieve(threadId, runId);
            
            if (run.status === 'completed') {
                // ç²å–å›æ‡‰
                const messages = await this.client.beta.threads.messages.list(threadId);
                const lastMessage = messages.data[0];
                
                if (lastMessage.role === 'assistant' && lastMessage.content[0].type === 'text') {
                    return lastMessage.content[0].text.value;
                }
                return null;
            } else if (run.status === 'failed' || run.status === 'cancelled' || run.status === 'expired') {
                throw new Error(`Assistant run ${run.status}: ${run.last_error?.message || 'Unknown error'}`);
            }
            
            // ç­‰å¾… 2 ç§’å¾Œå†æª¢æŸ¥
            await this.sleep(2000);
        }
        
        throw new Error('Assistant run timed out');
    }

    /**
     * è§£æè©•åˆ†å›æ‡‰
     */
    private parseGradingResponse(responseText: string, criteria: any[]): GradingResultData {
        try {
            // ç§»é™¤å¯èƒ½çš„ markdown æ¨™è¨˜
            const cleanedText = responseText.replace(/```json\s*/g, '').replace(/```\s*/g, '').trim();
            const parsed = JSON.parse(cleanedText);
            
            // é©—è­‰å’Œæ§‹å»ºçµæœ
            return this.buildResultFromParsed(parsed, criteria);

        } catch (error) {
            logger.error('Failed to parse OpenAI response:', { responseText: responseText.substring(0, 500) + '...', error });

            // è¿”å›é è¨­çµæœ
            const maxScore = criteria.reduce((sum, c) => sum + (c.maxScore || 0), 0);
            return {
                totalScore: 0,
                maxScore,
                breakdown: criteria.map(criterion => ({
                    criteriaId: criterion.id,
                    score: 0,
                    feedback: '**è©•åˆ†å¤±æ•— - JSON è§£æéŒ¯èª¤**\n\nå¯èƒ½åŸå› ï¼šOpenAI å›æ‡‰æ ¼å¼éŒ¯èª¤æˆ–å…§å®¹è¢«æˆªæ–·ã€‚\n\nè«‹é‡è©¦æˆ–è¯ç¹«æŠ€è¡“æ”¯æ´ã€‚'
                })),
                overallFeedback: '**ç³»çµ±éŒ¯èª¤ - ç„¡æ³•å®Œæˆè©•åˆ†**\n\nOpenAI æœå‹™å›æ‡‰è§£æå¤±æ•—ï¼Œè«‹é‡è©¦æˆ–è¯ç¹«æŠ€è¡“æ”¯æ´ã€‚'
            };
        }
    }

    /**
     * å¾è§£æçš„ JSON æ§‹å»ºçµæœ
     */
    private buildResultFromParsed(parsed: any, criteria: any[]): GradingResultData {
        // é©—è­‰å¿…è¦æ¬„ä½
        if (!parsed.totalScore && parsed.totalScore !== 0) {
            parsed.totalScore = 0;
        }
        if (!parsed.maxScore && parsed.maxScore !== 0) {
            parsed.maxScore = criteria.reduce((sum, c) => sum + (c.maxScore || 0), 0);
        }
        if (!parsed.breakdown || !Array.isArray(parsed.breakdown)) {
            parsed.breakdown = [];
        }

        // ç¢ºä¿ breakdown åŒ…å«æ‰€æœ‰è©•åˆ†é …ç›®
        const result: GradingResultData = {
            totalScore: Math.round(parsed.totalScore),
            maxScore: Math.round(parsed.maxScore),
            breakdown: criteria.map(criterion => {
                const found = parsed.breakdown.find((item: any) =>
                    item.criteriaId === criterion.id || item.criteriaId === criterion.name
                );

                return {
                    criteriaId: criterion.id,
                    score: found ? Math.round(found.score) : 0,
                    feedback: found ? found.feedback : 'ç„¡è©³ç´°åˆ†æ'
                };
            }),
            overallFeedback: parsed.overallFeedback || 'ç„¡ç¶œåˆè©•åƒ¹'
        };

        return result;
    }

    /**
     * è¨ˆç®— tokenï¼ˆç°¡å–®ä¼°ç®—ï¼‰
     */
    private estimateTokens(text: string): number {
        return Math.ceil(text.length / 4);
    }

    /**
     * æŒ‡æ•¸é€€é¿é‡è©¦æ©Ÿåˆ¶
     */
    private async retryWithBackoff<T>(
        operation: () => Promise<T>,
        maxRetries: number = 3,
        baseDelay: number = 1000
    ): Promise<T> {
        let lastError: any;
        
        for (let attempt = 1; attempt <= maxRetries; attempt++) {
            try {
                logger.info(`ğŸ”‘ Attempting OpenAI API call (attempt ${attempt}/${maxRetries})`);
                return await operation();
            } catch (error: any) {
                lastError = error;
                const isRetryableError = this.isRetryableError(error);
                const isLastAttempt = attempt === maxRetries;
                
                logger.warn(`âŒ OpenAI API call failed:`, {
                    error: error.message,
                    status: error.status,
                    attempt,
                    maxRetries,
                    isRetryable: isRetryableError
                });
                
                if (!isRetryableError || isLastAttempt) {
                    throw error;
                }
                
                // è¨ˆç®—å»¶é²æ™‚é–“
                const delay = baseDelay * Math.pow(2, attempt - 1) + Math.random() * 1000;
                logger.warn(`â³ Retrying in ${delay.toFixed(0)}ms...`);
                await this.sleep(delay);
            }
        }
        
        throw lastError || new Error('Max retries exceeded');
    }

    /**
     * åˆ¤æ–·æ˜¯å¦ç‚ºå¯é‡è©¦çš„éŒ¯èª¤
     */
    private isRetryableError(error: any): boolean {
        if (!error) return false;
        
        const errorMessage = error.message || '';
        const statusCode = error.status || error.code;
        
        // 429 Too Many Requests
        if (statusCode === 429) return true;
        
        // 500+ Server Errors
        if (statusCode >= 500) return true;
        
        // ç‰¹å®šéŒ¯èª¤è¨Šæ¯
        const retryableMessages = [
            'timeout',
            'rate limit',
            'overloaded',
            'unavailable',
            'internal error'
        ];
        
        return retryableMessages.some(msg => 
            errorMessage.toLowerCase().includes(msg)
        );
    }

    /**
     * è¨˜éŒ„ API éŒ¯èª¤
     */
    private recordApiError(error: any): void {
        this.consecutiveErrors++;
        this.lastErrorTime = Date.now();
        logger.warn(`ğŸ“ˆ Consecutive OpenAI API errors: ${this.consecutiveErrors}/${this.MAX_CONSECUTIVE_ERRORS}`);
    }

    /**
     * è¨˜éŒ„ API æˆåŠŸ
     */
    private recordApiSuccess(): void {
        this.consecutiveErrors = 0;
        logger.info(`âœ… OpenAI API call successful`);
    }

    /**
     * éŒ¯èª¤åˆ†æ
     */
    private analyzeError(error: any): { userMessage: string; type: string; retryable: boolean } {
        const errorInfo = {
            userMessage: 'ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦æˆ–è¯ç¹«æŠ€è¡“æ”¯æ´ã€‚',
            type: 'Unknown',
            retryable: false
        };

        if (error instanceof Error) {
            errorInfo.userMessage = error.message;
            errorInfo.type = error.name;
            errorInfo.retryable = this.isRetryableError(error);
        }

        // é‡å° OpenAI ç‰¹å®šéŒ¯èª¤æä¾›å‹å–„è¨Šæ¯
        if (error.status === 429) {
            errorInfo.userMessage = 'ğŸš¦ OpenAI æœå‹™è«‹æ±‚éæ–¼é »ç¹ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚';
        } else if (error.status >= 500) {
            errorInfo.userMessage = 'ğŸ”§ OpenAI æœå‹™æš«æ™‚ä¸å¯ç”¨ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚';
        } else if (error.message?.includes('file')) {
            errorInfo.userMessage = 'ğŸ“ æª”æ¡ˆè™•ç†å¤±æ•—ï¼Œè«‹æª¢æŸ¥æª”æ¡ˆæ ¼å¼æ˜¯å¦æ­£ç¢ºã€‚';
        }

        return errorInfo;
    }

    /**
     * æ¸…ç†è³‡æº
     */
    private async cleanupResources(
        file: OpenAI.Files.FileObject | null,
        assistant: OpenAI.Beta.Assistants.Assistant | null,
        thread: OpenAI.Beta.Threads.Thread | null
    ): Promise<void> {
        try {
            // æ¸…ç†æª”æ¡ˆ
            if (file) {
                await this.client.files.del(file.id);
                logger.info(`ğŸ§¹ Cleaned up file: ${file.id}`);
            }
            
            // æ¸…ç† Assistant
            if (assistant) {
                await this.client.beta.assistants.del(assistant.id);
                logger.info(`ğŸ§¹ Cleaned up assistant: ${assistant.id}`);
            }
            
            // Thread æœƒè‡ªå‹•æ¸…ç†ï¼Œä¸éœ€è¦æ‰‹å‹•åˆªé™¤
        } catch (cleanupError) {
            logger.warn(`âš ï¸ Failed to cleanup OpenAI resources:`, cleanupError);
        }
    }

    /**
     * ç·Šæ€¥æ¸…ç†
     */
    private async emergencyCleanup(
        file: OpenAI.Files.FileObject | null,
        assistant: OpenAI.Beta.Assistants.Assistant | null,
        thread: OpenAI.Beta.Threads.Thread | null
    ): Promise<void> {
        logger.info(`ğŸš¨ Emergency cleanup of OpenAI resources`);
        await this.cleanupResources(file, assistant, thread);
    }

    /**
     * ç¡çœ å‡½æ•¸
     */
    private sleep(ms: number): Promise<void> {
        return new Promise(resolve => setTimeout(resolve, ms));
    }
}

let openaiService: OpenAIService | null = null;

export function getOpenAIService(): OpenAIService {
    if (!openaiService) {
        openaiService = new OpenAIService();
    }
    return openaiService;
}

export default OpenAIService; 