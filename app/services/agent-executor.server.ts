/**
 * Agent Executor for Grading (True Agent Pattern)
 *
 * Uses Vercel AI SDK ToolLoopAgent for autonomous grading.
 *
 * Key differences from previous phase-based approach:
 * - All tools available at once (LLM decides order)
 * - Natural thinking flow (no hardcoded phases)
 * - Stops when generate_feedback is called
 * - Transparent reasoning for UI display
 *
 * Inspired by Anthropic's "Building Effective Agents":
 * https://www.anthropic.com/engineering/building-effective-agents
 */

import { ToolLoopAgent, generateObject, streamText, type StepResult, type ToolSet } from 'ai';
import { createGoogleGenerativeAI } from '@ai-sdk/google';
import { z } from 'zod';
import { redis } from '@/lib/redis';

// ============================================================================
// MODEL PROVIDER CONFIGURATION
// ============================================================================

function createGeminiModel(apiKey: string) {
  const gemini = createGoogleGenerativeAI({ apiKey });
  // Gemini 3 Flash Preview (Correct ID: gemini-3-flash-preview)
  return gemini('gemini-2.5-flash');
}
import type {
  AgentGradingParams,
  AgentGradingResult,
  AgentStep,
  ParsedCriterion,
  ReferenceDocument,
} from '@/types/agent';
import { createAgentTools } from './agent-tools.server';
import logger from '@/utils/logger';
import { getKeyHealthTracker, type ErrorType } from './gemini-key-health.server';

// ============================================================================
// TYPES
// ============================================================================

interface GradingContext {
  rubricName: string;
  criteria: ParsedCriterion[];
  content: string;
  fileName: string;
  referenceDocuments?: ReferenceDocument[];
  assignmentTitle?: string;
  assignmentDescription?: string;
  assignmentType?: string;
  userLanguage?: string;
}

interface GradingLocaleText {
  noSpecificFeedback: string;
  priorityLabel: string;
  strengthsLabel: string;
  improvementsLabel: string;
  encouragementExcellent: string;
  encouragementGood: string;
  encouragementFair: string;
  encouragementNeedsWork: string;
  defaultOverallGood: string;
  defaultOverallNeedsWork: string;
  gradingCompletedFallback: string;
  totalScorePrefix: string;
  fallbackInterrupted: string;
  unableToAnalyze: string;
  pleaseResubmit: string;
  gradingInterrupted: string;
  analysisInterrupted: string;
}

function getGradingLocaleText(userLanguage?: string): GradingLocaleText {
  const isZh = (userLanguage || 'zh-TW').startsWith('zh');

  if (isZh) {
    return {
      noSpecificFeedback: 'ç„¡å…·é«”å›é¥‹',
      priorityLabel: 'å„ªå…ˆæ”¹é€²',
      strengthsLabel: 'å„ªé»',
      improvementsLabel: 'æ”¹é€²å»ºè­°',
      encouragementExcellent: 'è¡¨ç¾å„ªç•°ï¼ç¹¼çºŒä¿æŒï¼',
      encouragementGood: 'æ•´é«”è¡¨ç¾è‰¯å¥½ï¼Œä»æœ‰é€²æ­¥ç©ºé–“ã€‚',
      encouragementFair: 'è¡¨ç¾å°šå¯ï¼Œå»ºè­°åŠ å¼·ä»¥ä¸‹æ–¹é¢çš„å­¸ç¿’ã€‚',
      encouragementNeedsWork: 'å»ºè­°é‡æ–°æª¢è¦–ä½œæ¥­è¦æ±‚ï¼Œä¸¦é‡å°è©•åˆ†æ¨™æº–é€é …æ”¹é€²ã€‚',
      defaultOverallGood: 'æ•´é«”è¡¨ç¾è‰¯å¥½ï¼Œä»æœ‰é€²æ­¥ç©ºé–“ã€‚',
      defaultOverallNeedsWork: 'å»ºè­°é‡æ–°æª¢è¦–ä½œæ¥­è¦æ±‚ï¼Œä¸¦é‡å°è©•åˆ†æ¨™æº–é€é …æ”¹é€²ã€‚',
      gradingCompletedFallback: 'è©•åˆ†å·²å®Œæˆï¼Œè«‹åƒé–±å„é …ç›®çš„å›é¥‹ã€‚',
      totalScorePrefix: 'ç¸½åˆ†',
      fallbackInterrupted: 'è©•åˆ†éç¨‹æœªæ­£å¸¸å®Œæˆï¼ˆ3-Step Process Interruptedï¼‰ã€‚è«‹é‡æ–°å˜—è©¦ã€‚',
      unableToAnalyze: 'ç„¡æ³•åˆ†æ',
      pleaseResubmit: 'è«‹é‡æ–°æäº¤',
      gradingInterrupted: 'è©•åˆ†ä¸­æ–·',
      analysisInterrupted: 'è©•åˆ†ä¸­æ–·',
    };
  }

  return {
    noSpecificFeedback: 'No specific feedback provided',
    priorityLabel: 'Top Priority',
    strengthsLabel: 'Strengths',
    improvementsLabel: 'Suggestions for Improvement',
    encouragementExcellent: 'Excellent work. Keep it up!',
    encouragementGood: 'Overall performance is good, with room to improve.',
    encouragementFair: 'The work is acceptable, but targeted improvement is recommended.',
    encouragementNeedsWork: 'Please review the assignment requirements and improve each rubric area step by step.',
    defaultOverallGood: 'Overall performance is good, with room to improve.',
    defaultOverallNeedsWork: 'Please review the assignment requirements and improve each rubric area step by step.',
    gradingCompletedFallback: 'Grading is complete. Please review the feedback for each criterion.',
    totalScorePrefix: 'Total Score',
    fallbackInterrupted: 'Grading did not complete successfully (3-Step Process Interrupted). Please try again.',
    unableToAnalyze: 'Unable to analyze',
    pleaseResubmit: 'Please resubmit',
    gradingInterrupted: 'Grading interrupted',
    analysisInterrupted: 'Grading interrupted',
  };
}

// ============================================================================
// HELPER: Optimize Rubric with LLM
// ============================================================================

async function optimizeRubricWithLLM(
  model: any,
  rubricName: string,
  rawCriteria: ParsedCriterion[],
  userLanguage?: string
): Promise<ParsedCriterion[]> {
  logger.info('[Agent] Optimizing rubric...');
  const isZh = (userLanguage || 'zh-TW').startsWith('zh');

  const prompt = isZh
    ? `
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ•™è‚²è©•é‡å°ˆå®¶ã€‚å„ªåŒ–ä»¥ä¸‹è©•åˆ†æ¨™æº–ï¼Œä½¿å…¶å° AI è©•åˆ†åŠ©æ•™æ›´å…·é«”ã€å®¢è§€ä¸”å¯åŸ·è¡Œã€‚

åŸå§‹è©•åˆ†æ¨™æº–åç¨±ï¼š${rubricName}

è¦æ±‚ï¼š
1. ä¿ç•™ï¼šIDã€åç¨±ã€ç¸½åˆ†ä¸è®Š
2. æ“´å……èªªæ˜ï¼šå…·é«”çš„è§€å¯ŸæŒ‡æ¨™ï¼Œå‘Šè¨´ AI æ‡‰è©²å°‹æ‰¾ä»€éº¼è­‰æ“š
3. å„ªåŒ–ç­‰ç´šï¼šæ›´å…·é«”å€åˆ†ä¸åŒåˆ†æ•¸æ®µçš„å·®ç•°

è¼¸å…¥ï¼š${JSON.stringify(rawCriteria, null, 2)}
`
    : `
You are an expert in educational assessment. Improve the following rubric to make it more specific, objective, and actionable for AI grading assistants.

Original rubric name: ${rubricName}

Requirements:
1. Preserve ID, name, and max score.
2. Expand descriptions with concrete observable indicators and expected evidence.
3. Refine scoring levels to better differentiate performance bands.

Input: ${JSON.stringify(rawCriteria, null, 2)}
`;

  try {
    const { object: optimizedCriteria } = await generateObject({
      model,
      schema: z.array(
        z.object({
          criteriaId: z.string(),
          name: z.string(),
          description: z.string(),
          maxScore: z.number(),
          levels: z.array(z.object({ score: z.number(), description: z.string() })).optional(),
        })
      ),
      messages: [{ role: 'user', content: prompt }],
      temperature: 1.0,
      providerOptions: {
        google: {
          safetySettings: [
            { category: 'HARM_CATEGORY_HATE_SPEECH', threshold: 'BLOCK_ONLY_HIGH' },
            { category: 'HARM_CATEGORY_DANGEROUS_CONTENT', threshold: 'BLOCK_ONLY_HIGH' },
            { category: 'HARM_CATEGORY_HARASSMENT', threshold: 'BLOCK_ONLY_HIGH' },
            { category: 'HARM_CATEGORY_SEXUALLY_EXPLICIT', threshold: 'BLOCK_ONLY_HIGH' },
          ],
        },
      },
    });

    logger.info({
      originalCount: rawCriteria.length,
      optimizedCount: optimizedCriteria.length,
    }, '[Agent] Rubric optimized');

    return optimizedCriteria;
  } catch (error) {
    logger.warn({ err: error }, '[Agent] Rubric optimization failed, using original');
    return rawCriteria;
  }
}

// ============================================================================
// SYSTEM PROMPT (Unified, not phase-locked)
// ============================================================================

function buildGradingSystemPrompt(ctx: GradingContext, isDirectMode: boolean = false): string {
  const lang = ctx.userLanguage || 'zh-TW';
  const isZh = lang.startsWith('zh');
  const languageRule = isZh
    ? `ã€èªè¨€è¦ç¯„ã€‘æ‰€æœ‰å°å¤–å¯è¦‹æ–‡å­—ï¼ˆå«æ€è€ƒéç¨‹ã€reasoningã€messageToStudentã€analysisã€sparringQuestions.questionï¼‰å¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚é™¤å°ˆæœ‰åè©å¤–ï¼Œé¿å…è‹±æ–‡å¥å­ã€‚`
    : `[Language Rule] All user-visible text (including thinking process, reasoning, messageToStudent, analysis, and sparringQuestions.question) must be in English. Avoid Chinese sentences except unavoidable proper nouns or quotes.`;

  // Core role definition with sparringQuestions requirement upfront
  const baseRole = isZh
    ? `ã€é‡è¦ã€‘èª¿ç”¨ generate_feedback æ™‚ï¼ŒsparringQuestions æ¬„ä½ç‚ºå¿…å¡«ï¼ˆè‡³å°‘ 3 å€‹å•é¡Œï¼‰ã€‚
${languageRule}

---

ä½ æ˜¯ä¸€ä½å…·æœ‰ 15 å¹´ç¶“é©—çš„è³‡æ·±å­¸ç§‘æ•™å¸«ï¼Œå°ˆé•·æ–¼å¯«ä½œæ•™å­¸èˆ‡å½¢æˆæ€§è©•é‡ã€‚
ä½ ç†Ÿæ‚‰ Rubric-Based Assessmentã€SOLO Taxonomyã€Bloom's Taxonomyã€Diagnostic Feedback ç­‰æ•™è‚²è©•é‡æ–¹æ³•ã€‚
ä½ çš„è©•åˆ†é¢¨æ ¼åš´è¬¹ä½†å…·å»ºè¨­æ€§ï¼Œé‡è¦– Evidence-Based Assessmentï¼ˆè­‰æ“šæœ¬ä½è©•é‡ï¼‰ã€‚`
    : `ã€IMPORTANTã€‘When calling generate_feedback, sparringQuestions field is REQUIRED (minimum 3 questions).
${languageRule}

---

You are a senior subject teacher with 15 years of experience in writing instruction and formative assessment.
You are proficient in Rubric-Based Assessment, SOLO Taxonomy, Bloom's Taxonomy, and Diagnostic Feedback.
Your grading style is rigorous yet constructive, emphasizing evidence-based assessment.`;

  const assignmentInfo = ctx.assignmentTitle
    ? isZh
      ? `
## ä½œæ¥­è³‡è¨Š
- æ¨™é¡Œï¼š${ctx.assignmentTitle}
- èªªæ˜ï¼š${ctx.assignmentDescription || 'ç„¡'}
- æª”æ¡ˆï¼š${ctx.fileName}
`
      : `
## Assignment Info
- Title: ${ctx.assignmentTitle}
- Description: ${ctx.assignmentDescription || 'N/A'}
- File: ${ctx.fileName}
`
    : isZh
      ? `## æª”æ¡ˆï¼š${ctx.fileName}`
      : `## File: ${ctx.fileName}`;

  const rubricInfo = isZh
    ? `
## è©•åˆ†æ¨™æº– (Rubric)ï¼š${ctx.rubricName}
${ctx.criteria.map((c, i) => `${i + 1}. **${c.name}** (${c.maxScore}åˆ†): ${c.description}`).join('\n')}

## è©•é‡ç¶­åº¦åƒè€ƒ

### æ–‡ç« çµæ§‹
- **Cohesionï¼ˆéŠœæ¥ï¼‰**ï¼šé€£æ¥è©ã€æŒ‡ä»£è©ä½¿ç”¨
- **Coherenceï¼ˆé€£è²«ï¼‰**ï¼šæ®µè½é‚è¼¯é—œä¿‚
- **Discourse Markers**ï¼šè½‰æŠ˜è©ã€æ‰¿æ¥è©

### èªè¨€é‹ç”¨
- **Syntactic Complexityï¼ˆå¥æ³•è¤‡é›œåº¦ï¼‰**ï¼šå¥å‹è®ŠåŒ–
- **Lexical Diversityï¼ˆè©å½™è±å¯Œåº¦ï¼‰**ï¼šç”¨è©ç²¾æº–åº¦
- **Mechanicsï¼ˆæ›¸å¯«è¦ç¯„ï¼‰**ï¼šæ¨™é»ã€æ ¼å¼

### å…§å®¹æ·±åº¦ï¼ˆSOLO Taxonomyï¼‰
Prestructuralï¼ˆé›¢é¡Œï¼‰â†’ Unistructuralï¼ˆå–®é»ï¼‰â†’ Multistructuralï¼ˆå¤šé»ç„¡æ•´åˆï¼‰â†’ Relationalï¼ˆæ•´åˆï¼‰â†’ Extended Abstractï¼ˆæ‰¹åˆ¤åæ€ï¼‰

### è­‰æ“šé‹ç”¨
- **Specificityï¼ˆå…·é«”æ€§ï¼‰**
- **Elaborationï¼ˆé—¡è¿°æ·±åº¦ï¼‰**
- **Evidence-Claim Alignmentï¼ˆè­‰æ“š-è«–é»å°æ‡‰ï¼‰**
`
    : `
## Rubric: ${ctx.rubricName}
${ctx.criteria.map((c, i) => `${i + 1}. **${c.name}** (${c.maxScore} points): ${c.description}`).join('\n')}

## Assessment Dimensions

### Structure
- **Cohesion**: use of connectors and references
- **Coherence**: paragraph logic and flow
- **Discourse Markers**: transitions and signposting

### Language Use
- **Syntactic Complexity**: sentence variety and control
- **Lexical Diversity**: precision and richness of vocabulary
- **Mechanics**: punctuation and formatting

### Depth of Understanding (SOLO Taxonomy)
Prestructural -> Unistructural -> Multistructural -> Relational -> Extended Abstract

### Use of Evidence
- **Specificity**
- **Elaboration**
- **Evidence-Claim Alignment**
`;

  const coreInstructions = isZh
    ? `
## è©•é‡åŸå‰‡

1. **Evidence-Based Scoring**ï¼šå¼•ç”¨åŸæ–‡è­‰æ“šï¼ˆç”¨ã€Œã€æ¨™ç¤ºï¼‰ï¼Œæ¯å€‹åˆ†æ•¸éœ€ Justification
2. **Criterion-Referenced**ï¼šå°ç…§ Rubric çµ¦åˆ†ï¼Œé è¨­ 80% æ»¿åˆ†ï¼Œé” Exemplary æ‰çµ¦æ»¿åˆ†
3. **Diagnostic Feedback**ï¼šé€²è¡Œ Error Analysisï¼Œæä¾› Revision Strategy
4. **Authentic Context**ï¼šé€™æ˜¯çœŸå¯¦å­¸ç”Ÿä½œæ¥­ï¼Œç›´æ¥è©•åˆ†ï¼Œä¸è¦èªªã€Œå‡è¨­ã€
5. **Concise**ï¼šé¿å…é‡è¤‡ï¼ŒFeedback è¦ç²¾æº–

## generate_feedback æ¬„ä½èªªæ˜

### overallFeedbackã€å¿…å¡«ã€‘
çµ¦å­¸ç”Ÿçœ‹çš„æ•´é«”å›é¥‹ï¼Œ2-4 å¥è©±ï¼Œèªæ°£æº«æš–åƒç­å°å¸«ï¼š
- æ•´é«”è¡¨ç¾ç¸½è©•
- æœ€å¤§å„ªé»
- æœ€éœ€æ”¹é€²é»
- ä¸€å¥é¼“å‹µ

### reasoningï¼ˆçµ¦æ•™å¸«ï¼‰
å®Œæ•´çš„å°ˆæ¥­è©•åˆ†æ¨ç†ï¼Œæ ¼å¼ç¯„ä¾‹ï¼š
\`\`\`
ã€è«–é»ç™¼å±• - 3/5 åˆ†ã€‘(SOLO: Multistructural)
Text Evidenceï¼šã€Œå¤§å®¶éƒ½åœ¨æ»‘æ‰‹æ©Ÿ...ã€
Error Analysisï¼šOversimplificationï¼Œæœªå€åˆ† Physical/Psychological Presence
Revision Strategyï¼šå¼•ç”¨ Sherry Turkleã€ŒAlone Togetherã€æ¦‚å¿µ
\`\`\`

### çµ¦å­¸ç”Ÿçš„æ¬„ä½
- **messageToStudent**: æº«æš–èªæ°£ï¼Œåƒç­å°å¸«é¼“å‹µ
- **topPriority**: æœ€éœ€æ”¹é€²çš„ä¸€ä»¶äº‹
- **encouragement**: æ‰¾å‡ºå€¼å¾—è‚¯å®šçš„é»

### criteriaScoresï¼ˆæ¯é …è©•åˆ†ï¼‰
- criteriaId, name, score, maxScore
- evidence: é—œéµå¼•ç”¨ï¼ˆæœ€å¤š 50 å­—ï¼‰
- analysis: ã€çµ¦å­¸ç”Ÿã€‘å£èªåŒ–å»ºè­°
- justification: ã€çµ¦æ•™å¸«ã€‘å°ˆæ¥­çµ¦åˆ†ç†ç”±

### æ•´é«”æ‘˜è¦
- overallObservation, strengths (2-3 å€‹), improvements (2-3 å€‹)

### sparringQuestionsã€å¿…å¡«ï¼Œ3 å€‹ã€‘
é‡å°å­¸ç”Ÿä½œæ¥­ç”Ÿæˆã€Œä¿ƒé€²åæ€ã€çš„å•é¡Œï¼ˆéç³¾éŒ¯å°å‘ï¼‰ï¼š

**è¨­è¨ˆåŸå‰‡**ï¼š
- ç›®æ¨™æ˜¯è®“å­¸ç”Ÿã€Œæƒ³æ·±ã€ï¼Œä¸æ˜¯è®“å­¸ç”Ÿã€Œæ”¹å°ã€
- æˆåŠŸçš„å°ç·´ = å­¸ç”Ÿé–‹å§‹è³ªç–‘è‡ªå·±çš„å‡è¨­ï¼Œæˆ–ç™¼ç¾æ¦‚å¿µä¸­çš„å¼µåŠ›
- ã€Œæˆ‘æ²’æƒ³éé€™å€‹ã€æ˜¯æ­£é¢çµæœï¼ˆAporiaï¼‰

**ç­–ç•¥ä½¿ç”¨æŒ‡å¼•**ï¼š
- ã€Œwarrant_probeã€(L2): è¿½å•ç†ç”± â€” ä½ èªª X è®“ä½ æ»¿è¶³ï¼Œç‚ºä»€éº¼ï¼Ÿ
- ã€Œevidence_checkã€(L2): æŸ¥è­‰ä¾†æº â€” é€™å€‹æ•¸æ“šæ˜¯å¾å“ªè£¡ä¾†çš„ï¼Ÿ
- ã€Œlogic_gapã€(L3): é‚è¼¯è·³èº â€” å¾ A åˆ° Bï¼Œä¸­é–“å°‘äº†ä»€éº¼ï¼Ÿ
- ã€Œcounter_argumentã€(L3): åæ–¹è§€é» â€” æœ‰äººå¯èƒ½æœƒèªª...ä½ æ€éº¼çœ‹ï¼Ÿ
- ã€Œmetacognitiveã€(L3): å¯«ä½œé¸æ“‡ â€” ä½ ç‚ºä»€éº¼é¸æ“‡ç”¨é€™å€‹æ–¹å¼è¡¨é”ï¼Ÿ
- ã€Œconceptualã€(L4): æ¦‚å¿µè¾¯è­‰ â€” ã€ç†æƒ³ã€å°ä½ ä¾†èªªæ„å‘³è‘—ä»€éº¼ï¼Ÿ

**é™åˆ¶**ï¼š
- è‡³å°‘ 2 å€‹å•é¡Œå¿…é ˆæ˜¯ L3+ å±¤ç´š (logic_gap / counter_argument / metacognitive / conceptual)
- é¿å…åªå•ã€Œå¯ä»¥è¬›å…·é«”ä¸€é»å—ï¼Ÿã€é€™é¡ç´”æ¾„æ¸…å•é¡Œ

**provocation_strategy é¸é …**ï¼ševidence_check | logic_gap | counter_argument | warrant_probe | metacognitive | conceptual

## èªæ°£å°ç…§è¡¨

| æ¬„ä½ | å°è±¡ | èªæ°£ |
|-----|-----|-----|
| reasoning, justification | æ•™å¸« | å°ˆæ¥­è¡“èª |
| messageToStudent, analysis | å­¸ç”Ÿ | å£èªåŒ–ã€åƒè€å¸«èªªè©± |
| sparringQuestions.question | å­¸ç”Ÿ | æŒ‘æˆ°ä½†å‹å–„ |
`
    : `
## Assessment Principles

1. **Evidence-Based Scoring**: cite original text evidence and justify each score.
2. **Criterion-Referenced**: score against rubric criteria; reserve full marks for truly exemplary work.
3. **Diagnostic Feedback**: include error analysis and a practical revision strategy.
4. **Authentic Context**: this is a real student submission. Grade directly.
5. **Concise**: avoid repetition and keep feedback precise.

## generate_feedback Fields

### overallFeedback [REQUIRED]
Provide 2-4 warm mentor-like sentences covering overall performance, major strength, priority improvement, and encouragement.

### reasoning (teacher-facing)
Provide professional grading rationale with evidence and revision strategy.

### student-facing fields
- **messageToStudent**: warm and supportive
- **topPriority**: one highest-priority improvement
- **encouragement**: one genuinely positive point

### criteriaScores (each criterion)
- criteriaId, name, score, maxScore
- evidence: key quote (max 50 words)
- analysis: student-friendly recommendation
- justification: teacher-facing scoring rationale

### overall summary
- overallObservation, strengths (2-3), improvements (2-3)

### sparringQuestions [REQUIRED, 3 items]
Create reflection-promoting questions (not just correction prompts).

**Constraints**:
- At least 2 questions must be L3+ (logic_gap / counter_argument / metacognitive / conceptual)
- Avoid shallow clarification-only prompts

**provocation_strategy options**: evidence_check | logic_gap | counter_argument | warrant_probe | metacognitive | conceptual

## Tone Matrix

| Field | Audience | Tone |
|-----|-----|-----|
| reasoning, justification | Teacher | Professional |
| messageToStudent, analysis | Student | Conversational and supportive |
| sparringQuestions.question | Student | Challenging but friendly |
`;

  const workflowInstructions = isZh
    ? `
## è©•åˆ†æµç¨‹ï¼ˆReAct æ¨¡å¼ï¼‰

### æ ¸å¿ƒåŸå‰‡ï¼šThink First, Act Later

1. **[Text]** å…ˆè¼¸å‡ºç´”æ–‡å­—åˆ†æï¼ˆæœƒå³æ™‚é¡¯ç¤ºçµ¦ä½¿ç”¨è€…ï¼‰
2. **[Action]** åˆ†æå®Œç•¢å¾Œå‘¼å«å·¥å…·

### åš´æ ¼å€åˆ†

**Text Outputï¼ˆæ€è€ƒéç¨‹ï¼‰**
- ç”¨é€”ï¼šæ·±åº¦åˆ†æã€æ‰¾è­‰æ“šã€æ¨ç†
- ç¦æ­¢ï¼šä¸è¦åœ¨æ–‡å­—ä¸­åŒ…å« JSON

**Tool Callï¼ˆè¡Œå‹•ï¼‰**
- reasoning æ¬„ä½ï¼šæç…‰ Text ä¸­çš„é—œéµç™¼ç¾ï¼ˆå‹¿é€å­—è¤‡è£½ï¼‰

### åŸ·è¡Œé †åº

1. **è¼¸å‡ºæ€è€ƒ**ï¼šé–±è®€ä½œæ¥­ï¼Œé€é …å°ç…§ Rubricï¼Œå¼•ç”¨åŸæ–‡
2. **calculate_confidence**ï¼šèªªæ˜åˆæ­¥åˆ†æèˆ‡ä¿¡å¿ƒ
3. **generate_feedback**ï¼šæç…‰å®Œæ•´è©•åˆ†ï¼ˆå« 3 å€‹ sparringQuestionsï¼‰

**é‡è¦**ï¼šå¿…é ˆå…ˆè¼¸å‡ºæ–‡å­—å†å‘¼å«å·¥å…·ã€‚ç›´æ¥å‘¼å«å·¥å…·æ˜¯éŒ¯èª¤çš„ã€‚
`
    : `
## Grading Workflow (ReAct)

1. Output plain-text analysis first.
2. Call tools after analysis.

Do not put JSON in plain-text thinking output.
Execution order: think -> calculate_confidence -> generate_feedback.
`;

  const relevanceCheck = isZh
    ? `
## ä»»å‹™ç›¸é—œæ€§æª¢æŸ¥

å¦‚æœåˆ¤å®šç‚ºé›¢é¡Œå›æ‡‰ï¼ˆOff-Topicï¼‰ï¼š
1. reasoning ä½¿ç”¨ï¼šã€Œæ­¤å›æ‡‰ç‚º Prestructural Level - å®Œå…¨é›¢é¡Œã€
2. æ‰€æœ‰è©•åˆ†é …ç›®çµ¦ 0 åˆ†
3. èªªæ˜ Task Alignment å•é¡Œ
`
    : `
## Off-topic Check

If the submission is off-topic:
1. Use reasoning: "This response is Prestructural level and off-topic."
2. Assign 0 to all rubric criteria.
3. Explain the task-alignment issue.
`;

  if (isDirectMode) {
    return `${baseRole}\n${assignmentInfo}\n${rubricInfo}\n${coreInstructions}\n${relevanceCheck}`;
  }

  return `${baseRole}\n${assignmentInfo}\n${rubricInfo}\n${coreInstructions}\n${workflowInstructions}\n${relevanceCheck}`;
}

// ============================================================================
// FALLBACK: Build result from steps when generate_feedback wasn't called
// ============================================================================

function buildFallbackResultFromSteps(
  steps: AgentStep[],
  criteria: ParsedCriterion[],
  userLanguage?: string
): any | null {
  logger.warn('[Agent Fallback] 3-Step Process interrupted. No intermediate scores available.');
  const localeText = getGradingLocaleText(userLanguage);

  // In the new 3-step process, scores are only generated in the final step.
  // If we are here, it means generate_feedback was not called or failed.

  return {
    totalScore: 0,
    maxScore: criteria.reduce((sum, c) => sum + c.maxScore, 0),
    overallFeedback: localeText.fallbackInterrupted,
    strengths: [localeText.unableToAnalyze],
    improvements: [localeText.pleaseResubmit],
    criteriaScores: criteria.map((c) => ({
      criteriaId: c.criteriaId,
      name: c.name,
      score: 0,
      maxScore: c.maxScore,
      evidence: localeText.gradingInterrupted,
      analysis: localeText.analysisInterrupted,
      justification: 'Process interrupted before generate_feedback',
    })),
    reasoning: 'The agent failed to complete the grading process.',
  };
}

// ============================================================================
// STOP CONDITION: When generate_feedback is called
// ============================================================================

function createStopCondition(maxSteps: number) {
  return (params: { steps: any[] }) => {
    // Stop if generate_feedback was called
    for (const step of params.steps) {
      if (step.toolCalls) {
        for (const call of step.toolCalls) {
          if (call.toolName === 'generate_feedback') {
            logger.info('[Agent] Stop: generate_feedback called');
            return true;
          }
        }
      }
    }

    // Safety: stop if max steps reached
    if (params.steps.length >= maxSteps) {
      logger.warn({ steps: params.steps.length }, '[Agent] Stop: max steps reached');
      return true;
    }

    return false;
  };
}

// ============================================================================
// DIRECT GRADING SCHEMA
// ============================================================================

const DirectGradingSchema = z.object({
  reasoning: z.string().describe('Complete grading rationale with criterion-level analysis'),
  messageToStudent: z.string().describe('Student-facing feedback in a warm and supportive tone'),
  topPriority: z.string().describe('The single highest-priority improvement for the student'),
  encouragement: z.string().describe('A short encouraging message for the student'),
  criteriaScores: z.array(
    z.object({
      criteriaId: z.string(),
      name: z.string(),
      score: z.number().describe('Score value'),
      maxScore: z.number(),
      evidence: z.string().describe('Direct evidence quote from the submission'),
      analysis: z.string().optional().describe('Student-facing suggestion'),
      justification: z.string().optional().describe('Teacher-facing scoring justification'),
    })
  ),
  overallObservation: z.string().describe('Overall observation'),
  strengths: z.array(z.string()).optional().describe('List of strengths'),
  improvements: z.array(z.string()).optional().describe('List of improvement suggestions'),
  // Sparring Questions for Productive Friction - REQUIRED!
  sparringQuestions: z.array(
    z.object({
      related_rubric_id: z.string().describe('Related rubric criterion ID'),
      target_quote: z.string().describe('Specific quote from the student submission'),
      provocation_strategy: z.enum(['evidence_check', 'logic_gap', 'counter_argument', 'warrant_probe', 'metacognitive', 'conceptual']).describe('Reflection strategy: L2 (warrant_probe, evidence_check) or L3+ (logic_gap, counter_argument, metacognitive, conceptual)'),
      question: z.string().describe('Reflection-promoting question for the student'),
      ai_hidden_reasoning: z.string().describe('Internal reasoning for question design'),
    })
  ).min(1).describe('[Required] Reflection sparring questions for the student (minimum 3, with L3+ depth)'),
});

// ============================================================================
// MAIN EXECUTOR: True Agent Pattern (ToolLoopAgent)
// ============================================================================

export async function executeGradingAgent(params: AgentGradingParams): Promise<AgentGradingResult> {
  const startTime = Date.now();
  const steps: AgentStep[] = [];
  const healthTracker = getKeyHealthTracker();
  let selectedKeyId: string | null = null;
  const isZh = (params.userLanguage || 'zh-TW').startsWith('zh');
  const localeText = getGradingLocaleText(params.userLanguage);

  try {
    logger.info({
      resultId: params.resultId,
      rubricName: params.rubricName,
      hasAssignmentTitle: !!params.assignmentTitle,
    }, '[Agent] Starting autonomous grading (ToolLoopAgent)');

    // 1. Setup Model (Google Generative AI)
    let model: any;

    // Flexible key detection (supports 1, 2, or 3 keys)
    const availableKeyIds = ['1'];
    if (process.env.GEMINI_API_KEY2) availableKeyIds.push('2');
    if (process.env.GEMINI_API_KEY3) availableKeyIds.push('3');

    selectedKeyId = await healthTracker.selectBestKey(availableKeyIds);
    if (!selectedKeyId) throw new Error('All Gemini API keys are throttled');

    const apiKey =
      selectedKeyId === '1'
        ? process.env.GEMINI_API_KEY
        : selectedKeyId === '2'
          ? process.env.GEMINI_API_KEY2
          : process.env.GEMINI_API_KEY3;
    if (!apiKey) throw new Error(`API key not found for keyId: ${selectedKeyId}`);

    model = createGeminiModel(apiKey);

    // 2. Optimize Rubric
    let effectiveCriteria = params.criteria;
    try {
      effectiveCriteria = await optimizeRubricWithLLM(
        model,
        params.rubricName,
        params.criteria,
        params.userLanguage
      );
    } catch (e) {
      logger.warn({ err: e }, '[Agent] Rubric optimization failed, using original');
    }

    // 3. Build Context
    const ctx: GradingContext = {
      rubricName: params.rubricName,
      criteria: effectiveCriteria,
      content: params.content,
      fileName: params.fileName,
      referenceDocuments: params.referenceDocuments,
      assignmentTitle: params.assignmentTitle,
      assignmentDescription: params.assignmentDescription,
      assignmentType: params.assignmentType,
      userLanguage: params.userLanguage,
    };

    // CHECK FOR DIRECT GRADING MODE
    if (params.useDirectGrading) {
       logger.info('[Agent] Executing Direct Grading Mode (Manual Branch)');
       const systemPrompt = buildGradingSystemPrompt(ctx, true);
       const userMessage = isZh
         ? `è«‹è©•åˆ†ä»¥ä¸‹å­¸ç”Ÿä½œæ¥­ï¼š

     ${params.assignmentTitle ? `ã€ä½œæ¥­æ¨™é¡Œã€‘${params.assignmentTitle}` : ''}
     ${params.assignmentDescription ? `ã€ä½œæ¥­èªªæ˜ã€‘${params.assignmentDescription}` : ''}
     ã€å­¸ç”Ÿä½œæ¥­å…§å®¹ã€‘
     ${params.content}

     è«‹ç›´æ¥è¼¸å‡ºè©•åˆ†çµæœ JSONã€‚`
         : `Please grade the following student submission:

     ${params.assignmentTitle ? `[Assignment Title] ${params.assignmentTitle}` : ''}
     ${params.assignmentDescription ? `[Assignment Description] ${params.assignmentDescription}` : ''}
     [Student Submission]
     ${params.content}

     Please output the grading result in JSON directly.`;
 
       try {
         const { object: result, usage, providerMetadata } = await generateObject({
           model,
           schema: DirectGradingSchema,
           messages: [
             { role: 'system', content: systemPrompt },
             { role: 'user', content: userMessage },
           ],
           providerOptions: {
             google: {
               thinkingConfig: {
                 includeThoughts: true,
                 thinkingLevel: 'high',
               },
               safetySettings: [
                 { category: 'HARM_CATEGORY_HATE_SPEECH', threshold: 'BLOCK_ONLY_HIGH' },
                 { category: 'HARM_CATEGORY_DANGEROUS_CONTENT', threshold: 'BLOCK_ONLY_HIGH' },
                 { category: 'HARM_CATEGORY_HARASSMENT', threshold: 'BLOCK_ONLY_HIGH' },
                 { category: 'HARM_CATEGORY_SEXUALLY_EXPLICIT', threshold: 'BLOCK_ONLY_HIGH' },
               ],
             },
           },
         });
 
         // Capture Gemini Native Thinking
         let directThinking = '';
         const googleMetadata = providerMetadata?.google as any;
         if (googleMetadata?.thoughts) {
           directThinking = googleMetadata.thoughts as string;
           logger.info({ length: directThinking.length }, '[Agent] Captured Direct Mode Thinking');
         }
 
         // Stream thinking to Redis (Bridge format)
         if (params.sessionId && directThinking) {
           await redis.publish(
             `session:${params.sessionId}`,
             JSON.stringify({
               type: 'text-delta',
               content: directThinking,
             })
           );
         }
 
         // Construct a "fake" step for the direct execution to fit the AgentGradingResult structure
         const steps: AgentStep[] = [
           {
             stepNumber: 1,
             toolName: 'direct_grading',
             reasoning: directThinking || result.reasoning, // Prefer native thinking if available
             toolOutput: result,
             durationMs: Date.now() - startTime,
             timestamp: new Date(),
           },
         ];
 
         // Map to AIGradingResult format to satisfy type requirements
         const mappedData = {
           breakdown: result.criteriaScores.map((s: any) => ({
             criteriaId: s.criteriaId,
             name: s.name,
             score: s.score,
             feedback: s.analysis || s.justification || '',
           })),
           overallFeedback: result.messageToStudent || result.overallObservation,
           summary: result.overallObservation,
           // Include sparring questions for Productive Friction
           sparringQuestions: result.sparringQuestions || [],
         };
 
         // Stream finish to Redis (Bridge format) with telemetry for thesis research
         if (params.sessionId) {
           const directExecutionTimeMs = Date.now() - startTime;
           await redis.publish(
             `session:${params.sessionId}`,
             JSON.stringify({
               type: 'finish',
               result: mappedData,
               // Telemetry for thesis data analysis
               meta: {
                 executionTimeMs: directExecutionTimeMs,
                 totalTokens: usage?.totalTokens || 0,
                 modelName: 'gemini-2.5-flash',
                 sparringQuestionsCount: mappedData.sparringQuestions?.length || 0,
                 mode: 'direct',
               }
             })
           );
         }
 
         return {
           success: true,
           data: mappedData,
           steps,
           confidenceScore: 1.0, // Direct mode assumes high confidence or N/A
           requiresReview: false,
           totalTokens: usage?.totalTokens || 0,
           executionTimeMs: Date.now() - startTime,
         };
       } catch (error) {
         logger.error({ err: error }, '[Agent] Direct grading failed');
         throw error;
       }
    }

    // 4. Create Tools
    const tools = createAgentTools({
      referenceDocuments: params.referenceDocuments,
      currentContent: params.content,
      assignmentType: params.assignmentType,
      sessionId: params.sessionId,
      userLanguage: params.userLanguage,
    });

    // 5. Execute Agent (ToolLoopAgent)
    
    const userMessage = isZh
      ? `è«‹è©•åˆ†ä»¥ä¸‹å­¸ç”Ÿä½œæ¥­ï¼š

    ${params.assignmentTitle ? `ã€ä½œæ¥­æ¨™é¡Œã€‘${params.assignmentTitle}` : ''}
    ${params.assignmentDescription ? `ã€ä½œæ¥­èªªæ˜ã€‘${params.assignmentDescription}` : ''}
    ã€å­¸ç”Ÿä½œæ¥­å…§å®¹ã€‘
    ï¼ˆæ³¨æ„ï¼šé€™æ˜¯çœŸå¯¦å­¸ç”Ÿçš„æäº¤ï¼Œè«‹ç›´æ¥è©•åˆ†ï¼Œä¸è¦å‡è¨­å®ƒæ˜¯ç¯„ä¾‹ï¼‰
    ${params.content}
    `
      : `Please grade the following student submission:

    ${params.assignmentTitle ? `[Assignment Title] ${params.assignmentTitle}` : ''}
    ${params.assignmentDescription ? `[Assignment Description] ${params.assignmentDescription}` : ''}
    [Student Submission]
    (Note: This is a real student submission. Grade directly and do not treat it as a hypothetical sample.)
    ${params.content}
    `;

    logger.info({
      contentLength: params.content.length,
      hasTitle: !!params.assignmentTitle,
    }, '[Agent] Executing ToolLoopAgent');

    let stepCounter = 0;
    let confidenceCalled = false;
    let feedbackCalled = false;
    let thinkCalled = false;  // NEW: Track if think was called

    const agent = new ToolLoopAgent({
      model,
      instructions: buildGradingSystemPrompt(ctx),
      tools,
      prepareStep: async ({ steps: agentSteps }) => {
        stepCounter++;
        
        // Use the agent's internal steps array to check tool completion status
        // This is synchronous - the data is available immediately unlike our async stream handler
        const completedToolNames = new Set<string>();
        if (agentSteps && agentSteps.length > 0) {
          for (const step of agentSteps) {
            if (step.toolResults) {
              for (const result of step.toolResults) {
                completedToolNames.add(result.toolName);
              }
            }
          }
        }
        
        const hasThinkAloud = completedToolNames.has('think_aloud');
        const hasConfidence = completedToolNames.has('calculate_confidence');
        const hasFeedback = completedToolNames.has('generate_feedback');
        
        logger.info({ 
          agentStepsCount: agentSteps?.length || 0,
          hasThinkAloud, 
          hasConfidence, 
          hasFeedback,
          completedTools: Array.from(completedToolNames)
        }, `[Agent] prepareStep ${stepCounter}`);
        
        // STEP 0: If feedback already generated, we're done - no more steps needed
        if (hasFeedback) {
          logger.info('[Agent] generate_feedback completed, signaling stop via toolChoice: none');
          return { toolChoice: 'none' as const };
        }
        
        // Force think_aloud tool on first step
        if (!hasThinkAloud) {
          logger.info('[Agent] Forcing think_aloud tool on first step');
          return {
            toolChoice: { type: 'tool' as const, toolName: 'think_aloud' }
          };
        }
        
        // STEP 2: After thinking, allow confidence calculation
        if (hasThinkAloud && !hasConfidence) {
          logger.info('[Agent] Allowing calculate_confidence after thinking');
          return { toolChoice: 'auto' };  // Let model choose when to calculate confidence
        }
        
        // STEP 3: After confidence, force generate_feedback
        if (hasConfidence && !hasFeedback) {
          logger.info('[Agent] Forcing generate_feedback after calculate_confidence');
          return {
            toolChoice: { type: 'tool', toolName: 'generate_feedback' }
          };
        }
        
        // Default: allow any tool
        return { toolChoice: 'auto' };
      },
      stopWhen: ({ steps: agentSteps }) => {
        // Safety: stop if max steps reached
        if (stepCounter >= 10) {
          logger.warn('[Agent] Max steps reached, stopping');
          return true;
        }
        
        // Check if generate_feedback has completed (has toolResults, not just toolCalls)
        if (agentSteps && agentSteps.length > 0) {
          for (const step of agentSteps) {
            if (step.toolResults) {
              for (const result of step.toolResults) {
                if (result.toolName === 'generate_feedback') {
                  logger.info('[Agent] generate_feedback has toolResult, stopping');
                  return true;
                }
              }
            }
          }
        }
        
        return false;
      },
    });

    const stream = await agent.stream({
      messages: [{ role: 'user', content: userMessage }],
    });

    let finalResult: any = null;
    let confidenceData: any = null;
    let currentThinking = '';
    
    // Token tracking
    let totalPromptTokens = 0;
    let totalCompletionTokens = 0;
    let totalTokens = 0;

    for await (const part of stream.fullStream) {
      // 0. Handle Token Usage - check all event types that might carry usage
      if ('usage' in part && part.usage) {
        const usage = part.usage as any;
        const stepTokens = usage.totalTokens || 0;
        
        totalPromptTokens += usage.promptTokens || 0;
        totalCompletionTokens += usage.completionTokens || 0;
        totalTokens += stepTokens;
        
        // Determine which step this belongs to (from most recent tool call)
        const currentStep = steps.length > 0 ? steps[steps.length - 1] : null;
        const toolName = currentStep?.toolName || 'unknown';
        
        logger.info({
          stepNumber: steps.length,
          toolName,
          promptTokens: usage.promptTokens,
          completionTokens: usage.completionTokens,
          stepTotal: stepTokens,
          cumulativeTotal: totalTokens,
        }, `[Agent] ğŸ“Š Step ${steps.length} Token Usage (${toolName})`);
      }
    
      // 1. Handle Text (Thinking)
      if (part.type === 'text-delta') {
        const text = part.text;
        currentThinking += text;
        
        // Stream to Redis (Bridge format)
        if (params.sessionId) {
          await redis.publish(
            `session:${params.sessionId}`,
            JSON.stringify({
              type: 'text-delta',
              content: text,
            })
          );
        }
      }

      // 2. Handle Tool Calls
      if (part.type === 'tool-call') {
        // ğŸ” Debug: Log generate_feedback tool call args
        if (part.toolName === 'generate_feedback') {
          const args = part.input as any;
          logger.info(`ğŸ” [Agent] generate_feedback ARGS - has sparringQuestions: ${!!args?.sparringQuestions}, count: ${args?.sparringQuestions?.length || 0}`);
          if (args?.sparringQuestions && args.sparringQuestions.length > 0) {
            logger.info(`ğŸ” [Agent] sparringQuestions[0] in args: ${JSON.stringify(args.sparringQuestions[0]).substring(0, 300)}`);
          }
          
          // =========================================================
          // âœ… [FIX] Early Capture: Save args as fallback in case 
          // tool-result is never received (defense-in-depth)
          // =========================================================
          if (args && args.criteriaScores && args.sparringQuestions?.length > 0) {
            logger.info('[Agent] ğŸŸ¢ Early Capture: Saving generate_feedback args as fallback');
            
            // Compute the same fields that the tool's execute function computes
            const totalScore = args.criteriaScores.reduce((sum: number, c: any) => sum + c.score, 0);
            const maxScore = args.criteriaScores.reduce((sum: number, c: any) => sum + c.maxScore, 0);
            const percentage = maxScore > 0 ? (totalScore / maxScore) * 100 : 0;
            
            // Transform criteriaScores to breakdown format
            const breakdown = args.criteriaScores.map((c: any) => ({
              criteriaId: c.criteriaId,
              name: c.name,
              score: c.score,
              feedback: c.analysis || c.justification || c.evidence || localeText.noSpecificFeedback,
            }));
            
            // Build overallFeedback from multiple sources (must match agent-tools execute logic)
            // Schema å¿…å¡«æ˜¯ overallFeedbackï¼ŒLLM å¸¸åªå¡«å®ƒè€Œæ²’å¡« messageToStudent/overallObservation
            let overallFeedback = (args.overallFeedback || args.messageToStudent || args.overallObservation || '').trim();
            if (args.topPriority) {
              overallFeedback += `\n\n**${localeText.priorityLabel}:**\n${args.topPriority}`;
            }
            if (args.strengths?.length > 0) {
              overallFeedback += `\n\n**${localeText.strengthsLabel}:**\n${args.strengths.map((s: string) => `- ${s}`).join('\n')}`;
            }
            if (args.improvements?.length > 0) {
              overallFeedback += `\n\n**${localeText.improvementsLabel}:**\n${args.improvements.map((i: string) => `- ${i}`).join('\n')}`;
            }
            if (args.encouragement?.trim()) {
              overallFeedback += `\n\n${args.encouragement.trim()}`;
            } else {
              if (percentage >= 90) overallFeedback += `\n\n${localeText.encouragementExcellent}`;
              else if (percentage >= 70) overallFeedback += `\n\n${localeText.encouragementGood}`;
              else if (percentage >= 50) overallFeedback += `\n\n${localeText.encouragementFair}`;
              else overallFeedback += `\n\n${localeText.encouragementNeedsWork}`;
            }
            const finalOverallFeedback = overallFeedback.trim() ||
              (percentage >= 70 ? localeText.defaultOverallGood : localeText.defaultOverallNeedsWork);

            // Only set as fallback if we don't already have a result
            // tool-result will override this if it arrives
            if (!finalResult) {
              finalResult = {
                reasoning: args.reasoning,
                breakdown,
                overallFeedback: finalOverallFeedback,
                totalScore,
                maxScore,
                percentage: Math.round(percentage),
                summary: `${localeText.totalScorePrefix}: ${totalScore}/${maxScore} (${percentage.toFixed(1)}%)`,
                sparringQuestions: args.sparringQuestions || [],
                // Mark as early capture for debugging
                _source: 'early_capture',
              };
              feedbackCalled = true;
              
              logger.info({
                totalScore,
                maxScore,
                sparringQuestionsCount: args.sparringQuestions?.length || 0,
              }, '[Agent] ğŸŸ¢ Early Capture complete');
            }
          }
        }
        
        // Stream tool call metadata only (no reasoning extraction)
        // Reasoning should come from native text-delta, not from tool args
        if (params.sessionId) {
          await redis.publish(
            `session:${params.sessionId}`,
            JSON.stringify({
              type: 'tool-call',
              toolCallId: part.toolCallId,
              toolName: part.toolName,
              args: part.input, // For observability in logs
            })
          );
        }
      }

      // 3. Handle Tool Results
      if (part.type === 'tool-result') {
        const toolName = part.toolName;
        const toolResult = part.output;

        logger.info(`[Agent] Tool completed: ${toolName}`);
        
        // ğŸ” Debug: Log generate_feedback tool result structure
        if (toolName === 'generate_feedback') {
          const resultObj = toolResult as any;
          logger.info(`ğŸ” [Agent] generate_feedback result keys: ${Object.keys(resultObj || {}).join(', ')}`);
          logger.info(`ğŸ” [Agent] generate_feedback has sparringQuestions: ${!!resultObj?.sparringQuestions}, count: ${resultObj?.sparringQuestions?.length || 0}`);
        }

        // Special handling for think/think_aloud tool: extract thought from args
        let stepReasoning = currentThinking || 'Tool Execution';
        if ((toolName === 'think' || toolName === 'think_aloud') && part.input) {
          // Extract thought/analysis from think tool args
          const thinkArgs = part.input as any;
          const content = thinkArgs.thought || thinkArgs.analysis;
          if (content) {
            stepReasoning = content;
            // DO NOT accumulate to currentThinking - the thought is already complete
            // and we don't want subsequent steps to inherit this reasoning
          }
        }

        steps.push({
          stepNumber: steps.length + 1,
          reasoning: stepReasoning, // Use extracted thought for think tool
          toolName: toolName,
          toolInput: part.input,
          toolOutput: toolResult,
          durationMs: 0,
          timestamp: new Date(),
        });
        
        // Reset thinking buffer for next step (except for think tool)
        if (toolName !== 'think') {
          currentThinking = '';
        }

        // Track tool calls for prepareStep logic (FIX for premature termination)
        if (toolName === 'think' || toolName === 'think_aloud') {
          thinkCalled = true;
        }
        if (toolName === 'calculate_confidence') {
          confidenceCalled = true;
          confidenceData = toolResult;
        }
        if (toolName === 'generate_feedback') {
          const typedResult = toolResult as any;
          
          // Only mark as completed if we actually have the required sparringQuestions
          // This allows the agent to retry if the tool threw an error or failed validation
          if (typedResult && Array.isArray(typedResult.sparringQuestions) && typedResult.sparringQuestions.length > 0) {
            // Check if we're overriding early capture
            const wasEarlyCapture = finalResult?._source === 'early_capture';
            if (wasEarlyCapture) {
              logger.info('[Agent] ğŸ”„ tool-result overriding early capture (preferred source)');
            }
            
            feedbackCalled = true;
            finalResult = toolResult;

            logger.info({
              hasSparringQuestions: true,
              sparringQuestionsCount: typedResult.sparringQuestions.length,
              resultKeys: Object.keys(toolResult || {}),
              source: 'tool_result',
            }, '[Agent] generate_feedback completed successfully');
          } else {
             logger.warn({ 
               toolResult: typeof toolResult === 'string' ? toolResult.substring(0, 100) : 'object' 
             }, '[Agent] generate_feedback failed validation (missing sparringQuestions or error)');
             // Do NOT set feedbackCalled = true, so the loop will retry
          }
        }
      }
    }

    if (!finalResult) {
      logger.warn('[Agent] âŒ No result captured (neither tool-result nor early-capture), building fallback...');
      finalResult = buildFallbackResultFromSteps(steps, params.criteria, params.userLanguage);
    } else if (finalResult._source === 'early_capture') {
      logger.info('[Agent] âš ï¸ Using early capture result (tool-result was not received)');
    } else {
      logger.info('[Agent] âœ… Using tool-result (preferred source)');
    }

    // 8. Build Response
    const executionTimeMs = Date.now() - startTime;
    await healthTracker.recordSuccess(selectedKeyId, executionTimeMs);

    // Ensure finalResult has breakdown
    if (finalResult && finalResult.criteriaScores && !finalResult.breakdown) {
      finalResult.breakdown = finalResult.criteriaScores.map((c: any) => ({
        criteriaId: c.criteriaId,
        name: c.name,
        score: c.score,
        feedback: c.analysis || c.justification || '',
      }));
    }

    // Ensure overallFeedback is never empty (early_capture or tool-result can leave it blank)
    if (finalResult && !(finalResult.overallFeedback && finalResult.overallFeedback.trim())) {
      finalResult.overallFeedback =
        finalResult.messageToStudent?.trim() ||
        finalResult.overallObservation?.trim() ||
        (finalResult.totalScore != null && finalResult.maxScore != null && finalResult.maxScore > 0
          ? (finalResult.totalScore / finalResult.maxScore) >= 0.7
            ? localeText.defaultOverallGood
            : localeText.defaultOverallNeedsWork
          : localeText.gradingCompletedFallback);
    }

    logger.info({
      totalSteps: steps.length,
      totalScore: finalResult?.totalScore,
      maxScore: finalResult?.maxScore,
      tokenUsage: {
        promptTokens: totalPromptTokens,
        completionTokens: totalCompletionTokens,
        total: totalTokens,
      },
      executionTimeMs,
    }, '[Agent] Grading completed');

    // Stream to Redis (Bridge format) with telemetry for thesis research
    if (params.sessionId) {
      await redis.publish(
        `session:${params.sessionId}`,
        JSON.stringify({
          type: 'finish',
          result: finalResult,
          // Telemetry for thesis data analysis
          meta: {
            executionTimeMs,
            totalTokens,
            modelName: 'gemini-2.5-flash',
            sparringQuestionsCount: finalResult?.sparringQuestions?.length || 0,
          }
        })
      );
    }

    // ğŸ” CRITICAL DEBUG: Check finalResult BEFORE returning
    logger.info(`ğŸ” [Agent Return] finalResult keys: ${Object.keys(finalResult || {}).join(', ')}`);
    logger.info(`ğŸ” [Agent Return] finalResult.sparringQuestions: ${finalResult?.sparringQuestions ? `YES (${finalResult.sparringQuestions.length})` : 'NO/UNDEFINED'}`);

    return {
      success: true,
      data: finalResult,
      steps,
      confidenceScore: confidenceData?.confidenceScore ?? 0.8,
      requiresReview: confidenceData?.shouldReview ?? false,
      totalTokens,  // Use tracked value instead of 0
      executionTimeMs,
    };
  } catch (error) {
    logger.error({ err: error }, '[Agent] Grading failed');

    // Stream to Redis (Bridge format)
    if (params.sessionId) {
      await redis.publish(
        `session:${params.sessionId}`,
        JSON.stringify({
          type: 'error',
          error: error instanceof Error ? error.message : String(error),
        })
      );
    }

    // Report failure to health tracker
    if (selectedKeyId) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      let errorType: ErrorType = 'other';

      if (errorMessage.includes('429') || errorMessage.includes('quota') || errorMessage.includes('limit')) {
        errorType = 'rate_limit';
      } else if (errorMessage.includes('503') || errorMessage.includes('overloaded')) {
        errorType = 'overloaded';
      }

      await healthTracker.recordFailure(selectedKeyId, errorType, errorMessage);
    }

    return {
      success: false,
      steps,
      confidenceScore: 0,
      requiresReview: true,
      totalTokens: 0,
      executionTimeMs: Date.now() - startTime,
      error: error instanceof Error ? error.message : String(error),
    };
  }
}

/**
 * Check if Agent grading is enabled
 */
export function isAgentGradingEnabled(): boolean {
  return process.env.USE_AGENT_GRADING === 'true';
}
